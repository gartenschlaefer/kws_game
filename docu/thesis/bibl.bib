% --
% my bibliography

% best quote ever
% It is trivial to design a machine that learns very quickly, does not generalize, and requires an enormous amount of hardware. 
% In fact this learning machine has already been built and is called a Random Access Memory.
% Y. LeCun

% project repository
@misc{KWSGame,
  author = {Christian Walter},
  title = {KWS Game},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/chrisworld/kws_game}},
  commit = {26ba289},
}


% --
% history

% perceptron I
@article{Rosenblatt1958,
  author = {F. Rosenblatt},
  title = {The Perceptron: A Probabilistic Model for Information Storage and Organization in The Brain},
  journal = {Psychological Review},
  pages = {65--386},
  year = {1958},
}

% perceptron II
@book{Rosenblatt1962,
  author = {Rosenblatt, Frank},
  title = {Principles of Neurodynamics: Perceptions and the Theory of Brain Mechanisms},
  publisher = {Spartan},
  year = {1962},
}

% backprop
@inbook{Rumelhart1986,
  author = {Rumelhart, David E. and McClelland, James L.},
  title = {Learning Internal Representations by Error Propagation}, 
  booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations}, 
  year = {1987},
  pages = {318-362},
  publisher = {MIT Press},
}

% backpropagation
@inproceedings{LeCun1986,
  author = {Le Cun, Yann},
  title = {Learning Process in an Asymmetric Threshold Network},
  booktitle = {Disordered Systems and Biological Organization},
  editor = {Bienenstock, E. and Souli{\'e}, F. Fogelman and Weisbuch, G.},
  year = {1986},
  publisher = {Springer Berlin Heidelberg},
  pages = {233--240},
}

% good intro to neural nets
@article{LeCun1998CnnGradient,
  author = {Y. {Lecun} and L. {Bottou} and Y. {Bengio} and P. {Haffner}},
  title = {Gradient-based learning applied to document recognition},
  journal = {Proceedings of the IEEE}, 
  year = {1998},
  volume = {86},
  number = {11},
  pages = {2278-2324},
}

% mfcc foundation
@article{Davis1980MFCC,
  author = {Davis, S. and Mermelstein, P.},
  title = {Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences}, 
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing}, 
  year = {1980},
  volume = {28},
  number = {4},
  pages = {357-366},
}

% plp
@article{Hermansky1987,
  author = {Hermansky,Hynek },
  title = {Perceptual linear predictive (PLP) analysis of speech},
  journal = {The Journal of the Acoustical Society of America},
  volume = {87},
  number = {4},
  pages = {1738-1752},
  year = {1990},
}

% plp vs mfcc
@inproceedings{Hoenig2005,
  author = {Florian H{\"{o}}nig and Georg Stemmer and Christian Hacker and Fabio Brugnara},
  title = {Revising Perceptual Linear Prediction (PLP)},
  booktitle = {Proceedings of the Interspeech Conference},
  pages = {2997-3000},
  year = {2005},
}

% mfcc inversion
@inproceedings{Boucheron2008,
  author = {Boucheron, Laura E. and De Leon, Phillip L.},
  booktitle = {2008 International Conference on Signals and Electronic Systems}, 
  title = {On the inversion of Mel-frequency cepstral coefficients for speech enhancement applications}, 
  pages = {485-488},
  year = {2008},
}

% statistical learning
@book{Vapnik1995,
  author = {Vapnik, Vladimir N.},
  title = {The Nature of Statistical Learning Theory},
  year = {1995},
  publisher = {Springer-Verlag},
}

% svm
@article{Cortes1995,
  author = {Cortes, Corinna and Vapnik, Vladimir},
  title = {Support-Vector Networks},
  journal = {Machine Learning},
  volume = {20},
  number = {3},
  pages = {273-297},
  year = {1995},
}

% deep learning imagenet
@inproceedings{Krizhevsky2012,
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  title = {ImageNet Classification with Deep Convolutional Neural Networks},
  booktitle = {Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)},
  volume = {25},
  year = {2012},
}

% rnn
@article{Staudenmeyer2019Lstm,
  author = {Ralf C. Staudemeyer and Eric Rothstein Morris},
  title = {Understanding {LSTM} - A tutorial into Long Short-Term Memory Recurrent Neural Networks}, 
  journal = {Preprint at arXiv:1909.09586},
  year = {2019},
}

% --
% general neural network techniques and theory

% dropout
@article{Hinton2012Dropout,
  author = {Geoffrey E. Hinton and Nitish Srivastava and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title = {Improving neural networks by preventing co-adaptation of feature detectors},
  journal = {Preprint at arXiv:1207.0580},
  year = {2012},
}

% adam optimizer
@inproceedings{Kingma2015Adam,
  author = {Diederik P. Kingma and Jimmy Ba},
  title = {Adam: A Method for Stochastic Optimization},
  booktitle = {Proceedings of the 3rd International Conference on Learning Representations (ICLR)},
  year = {2015},
}

% goodfellow deep learning book
@book{Goodfellow2016,
  title = {Deep Learning},
  author = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  publisher = {MIT Press},
  note = {\url{http://www.deeplearningbook.org}},
  year = {2016},
}

% energy based learning lecun
@inbook{LeCun2006,
  title = {A tutorial on energy-based learning},
  author = {Yann Lecun and Sumit Chopra and Raia Hadsell and Ranzato, {Marc Aurelio} and Huang, {Fu Jie}},
  year = {2006},
  language = {English (US)},
  editor = {G. Bakir and T. Hofman and B. Scholkopt and A. Smola and B. Taskar},
  booktitle = {Predicting structured data},
  publisher = {MIT Press},
}

% deep learning lecun
@online{DeepLearning, 
 author = {Yann LeCun and Alfredo Canziani and Mark Goldstein and Zeming Lin},
 title = {Deep Learning},
 year = {2021},
 url = {https://atcold.github.io/pytorch-Deep-Learning/},
 urldate = {2021-07-27},
}

% relu
@inproceedings{Zeiler2013Relu, 
  author = {Zeiler, M.D. and Ranzato, M. and Monga, R. and Mao, M. and Yang, K. and Le, Q.V. and Nguyen, P. and Senior, A. and Vanhoucke, V. and Dean, J. and Hinton, G.E.},
  title = {On rectified linear units for speech processing},
  booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing}, 
  year = {2013},
  pages = {3517-3521},
}

% --
% convolutional neural networks

% first paper about cnns
@inbook{LeCun1989Generalization,
  author = {Yann Lecun},
  title = {Generalization and network design strategies},
  booktitle = {Connectionism in perspective},
  year = {1989},
}

% understanding CNNs
@inproceedings{Zeiler2013VisCNN,
  author = {Zeiler, Matthew D. and Fergus, Rob},
  title = {Visualizing and Understanding Convolutional Networks},
  booktitle = {Proceedings of the 13th European Conference on Computer Vision (ECCV)},
  pages = {818-833},
  year = {2014},
}


% --
% dataset

% speech commands dataset
@article{Warden2018SpeechCommands,
  title = {{Speech Commands}: A Dataset for Limited-Vocabulary Speech Recognition},
  author = {Pete Warden},
  journal = {Preprint at arXiv:1804.03209},
  year = {2018},
}


% --
% benchmark networks

% benchmark list
@online{PaperswithcodeKWS, 
 author = {Papers with Code},
 title = {keyword-spotting-on-google-speech-commands},
 year = {2021},
 url = {https://paperswithcode.com/sota/keyword-spotting-on-google-speech-commands},
 urldate = {2021-06-02},
}

% keyword transformer -> highes accuracy
@inproceedings{Berg2021KeywordTransformer,
  title = {{Keyword Transformer}: A Self-Attention Model for Keyword Spotting},
  author = {Axel Berg and Mark Oâ€™Connor and Miguel Tairum Cruz},
  booktitle = {Proceedings of the Interspeech Conference},
  pages = {4249-4253},
  year = {2021},
}


% --
% small-footprint or efficient

% sainath small footprint
@inproceedings{Sainath2015KWS,
  title = {Convolutional neural networks for small-footprint keyword spotting},
  author = {Tara N. Sainath and Carolina Parada},
  booktitle = {Proceedings of the Interspeech Conference},
  year = {2015},
}

% edge
@article{Zhang2017HelloEdge,
  author = {Yundong Zhang and Naveen Suda and Liangzhen Lai and Vikas Chandra},
  title = {{Hello Edge}: Keyword Spotting on Microcontrollers},
  journal = {Preprint at arXiv:1711.07128},
  year = {2017},
}

% tu graz, resource efficient
@inproceedings{Peter2020ResourceEffDNN,
  author = {Peter, David and Roth, Wolfgang and Pernkopf, Franz},
  title = {Resource-Efficient DNNs for Keyword Spotting using Neural Architecture Search and Quantization}, 
  booktitle = {Proceedings of the 25th International Conference on Pattern Recognition (ICPR)}, 
  pages = {9273-9279},
  year = {2021},
}

% deep residual
@inproceedings{Tang2017DeepRes,
  author = {Tang, Raphael and Lin, Jimmy},
  title = {Deep Residual Learning for Small-Footprint Keyword Spotting}, 
  booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  pages = {5484-5488},
  year = {2018},
}

% power consumption kws
@inproceedings{Tang2018PowerConsKWS,
  author = {Tang, Raphael and Wang, Weijie and Tu, Zhucheng and Lin, Jimmy},
  title = {An Experimental Analysis of the Power Consumption of Convolutional Neural Networks for Keyword Spotting}, 
  booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)}, 
  pages = {5479-5483},
  year = {2018},
}


% --
% game theory

% game theory
@book{VonNeumann1944,
  title = {Theory of Games and Economic Behavior},
  author = {John von Neumann and Oskar Morgenstern},
  publisher = {Princeton University Press},
  year = {1944},
}

% --
% adversarial, gan

% adv paper I
@inproceedings{Goodfellow2014GANs,
  author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  title = {Generative Adversarial Nets},
  booktitle = {Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)},
  volume = {27},
  year = {2014},
}

% radford dcgan pytorch tutorial
@inproceedings{Radford2016DCGAN,
  author = {Alec Radford and Luke Metz and Soumith Chintala},
  title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
  booktitle = {Proceedings of the 4th International Conference on Learning Representations (ICLR)},
  year = {2016},
}

% eeg adversarial
@article{Oezdenizci2020EEG,
  author = {Ã–zdenizci, Ozan and Wang, Ye and Koike-Akino, Toshiaki and ErdoÄŸmuÅŸ, Deniz},
  title = {Learning Invariant Representations From EEG via Adversarial Inference}, 
  journal = {IEEE Access}, 
  volume = {8},
  pages = {27074-27085},
  year = {2020},
}

% up convolution
@inproceedings{Durall2020UpConv,
  author = {Durall, Ricard and Keuper, Margret and Keuper, Janis},
  title = {Watch Your Up-Convolution: {CNN} Based Generative Deep Neural Networks Are Failing to Reproduce Spectral Distributions}, 
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  pages = {7887-7896},
  year = {2020},
}


% --
% wavenets

% wavenet
@article{Oord2016Wavenet,
  author = {A{\"{a}}ron van den Oord and Sander Dieleman and Heiga Zen and Karen Simonyan and Oriol Vinyals and Alex Graves and Nal Kalchbrenner and Andrew W. Senior and Koray Kavukcuoglu},
  title = {{WaveNet}: A Generative Model for Raw Audio},
  journal = {Preprint at arXiv:1609.03499},
  year = {2016},
}

% pytorch implementation example
@misc{Herrmann2018,
  author = {Vincent Herrmann},
  title = {pytorch-wavenet},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/vincentherrmann/pytorch-wavenet}},
  commit = {26ba289},
}

% wavenet for music (has changed the name)
@article{Zhang2020WavenetMusic,
  author = {Xulong Zhang and Yongwei Gao and Yi Yu and Wei Li},
  title = {Music Artist Classification with WaveNet Classifier for Raw Waveform Audio Data}, 
  journal = {Preprint at arXiv:2004.04371},
  year = {2020},
}

% fast wavenet
% https://github.com/tomlepaine/fast-wavenet


% --
% transfer learning

% info website:
% https://cs231n.github.io/transfer-learning/

@online{TransferLearning, 
  author = {Standford CS231 course},
  title = {Transfer Learning},
  year = {2021},
  url = {https://cs231n.github.io/transfer-learning/},
  urldate = {2021-07-01},
}


% --
% resnets

% tutorial
% https://pytorch-tutorial.readthedocs.io/en/latest/tutorial/chapter03_intermediate/3_2_2_cnn_resnet_cifar10/


% --
% kws in games

% kws children games
@inproceedings{Harshavardhan2015,
  title = {Keyword spotting in multi-player voice driven games for children},
  author = {S. Harshavardhan and J. Lehman and Rita Singh},
  booktitle = {Proceedings of the Interspeech Conference},
  year = {2015},
}

% sphinx
@inproceedings{Huggins2006PocketSphinx,
  author = {Huggins-Daines, D. and Kumar, M. and Chan, A. and Black, A.W. and Ravishankar, M. and Rudnicky, A.I.},
  booktitle = {Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing Proceedings}, 
  title = {Pocketsphinx: A Free, Real-Time Continuous Speech Recognition System for Hand-Held Devices}, 
  year = {2006},
  volume = {1},
  pages = {I-I},
  doi = {10.1109/ICASSP.2006.1659988},
}

% overview sphinx
@article{Lee1990OverviewSpinx,
  author = {Lee, K.-F. and Hon, H.-W. and Reddy, R.},
  title = {An overview of the SPHINX speech recognition system}, 
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing}, 
  volume = {38},
  number = {1},
  pages = {35-45},
  year = {1990},
  xxxdoi = {10.1109/29.45616},
  xxxmonth = {Jan},
  xxxkeywords = {Speech recognition;Hidden Markov models;Error analysis;Automatic speech recognition;Degradation;Vocabulary;Mars;System testing;Marine vehicles;Continuous wavelet transforms},
  xxxabstract = {A description is given of SPHINX, a system that demonstrates the feasibility of accurate, large-vocabulary, speaker-independent, continuous speech recognition. SPHINX is based on discrete hidden Markov models (HMMs) with LPC- (linear-predictive-coding) derived parameters. To provide speaker independence, knowledge was added to these HMMs in several ways: multiple codebooks of fixed-width parameters, and an enhanced recognizer with carefully designed models and word-duration modeling. To deal with coarticulation in continuous speech, yet still adequately represent a large vocabulary, two new subword speech units are introduced: function-word-dependent phone models and generalized triphone models. With grammars of perplexity 997, 60, and 20, SPHINX attained word accuracies of 71, 94, and 96%, respectively, on a 997-word task.<>},
}

% windows asr
@inproceedings{Xiong2017SpeechRecSystem,
  author = {Xiong, W. and Wu, L. and Alleva, F. and Droppo, J. and Huang, X. and Stolcke, A.},
  booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)}, 
  title = {The Microsoft 2017 Conversational Speech Recognition System}, 
  pages = {5934-5938},
  year = {2018},
}


% --
% signal processing

% fft
@article{Brigham1967FFT,
  author = {Brigham, E. O. and Morrow, R. E.},
  title = {The fast Fourier transform}, 
  journal = {IEEE Spectrum}, 
  volume = {4},
  number = {12},
  pages = {63-70},
  year = {1967},
}


% --
% frameworks

% pytorch
@inproceedings{Paszke2019Pytorch,
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  title = {{PyTorch}: An Imperative Style, High-Performance Deep Learning Library},
  booktitle = {Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)},
  volume = {32},
  year = {2019},
  xxxeditor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  xxxpublisher = {Curran Associates, Inc.},
}

% tensorflow
@misc{Tensorflow,
  author = { Mart\'{i}n Abadi and Ashish Agarwal and Paul Barham and Eugene Brevdo and Zhifeng Chen and Craig Citro and Greg S. Corrado and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Ian Goodfellow and Andrew Harp and Geoffrey Irving and Michael Isard and Yangqing Jia and Rafal Jozefowicz and Lukasz Kaiser and Manjunath Kudlur and Josh Levenberg and Dandelion Man\'{e} and Rajat Monga and Sherry Moore and Derek Murray and Chris Olah and Mike Schuster and Jonathon Shlens and Benoit Steiner and Ilya Sutskever and Kunal Talwar and Paul Tucker and Vincent Vanhoucke and Vijay Vasudevan and Fernanda Vi\'{e}gas and Oriol Vinyals and Pete Warden and Martin Wattenberg and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
  title = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  url = {https://www.tensorflow.org/},
  note = {Software available from tensorflow.org},
  year = {2015},
}


% --
% papers of remark

% Wav2KWS: Transfer Learning from Speech Representations for Keyword Spotting
% --
% my bibliography


% --
% history

% perceptron I
@ARTICLE{Rosenblatt1958,
  author = {F. Rosenblatt},
  title = {The Perceptron: A Probabilistic Model for Information Storage and Organization in The Brain},
  journal = {Psychological Review},
  year = {1958},
  pages = {65--386}
}

% perceptron II
@book{Rosenblatt1962,
  author        = "Rosenblatt, Frank",
  title         = "{Principles of neurodynamics: perceptions and the theory of brain mechanisms}",
  publisher     = "Spartan",
  address       = "Washington, DC",
  year          = "1962",
  url           = "https://cds.cern.ch/record/239697"
}

% backpropagation
@inproceedings{Rumelhart1986,
  title={Learning internal representations by error propagation},
  author={D. Rumelhart and Geoffrey E. Hinton and R. J. Williams},
  year={1986}
}

% backpropagation
@InProceedings{LeCun1986,
author="Le Cun, Yann",
editor="Bienenstock, E.
and Souli{\'e}, F. Fogelman
and Weisbuch, G.",
title="Learning Process in an Asymmetric Threshold Network",
booktitle="Disordered Systems and Biological Organization",
year="1986",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="233--240",
abstract="Threshold functions and related operators are widely used as basic elements of adaptive and associative networks [Nakano 72, Amari 72, Hopfield 82]. There exist numerous learning rules for finding a set of weights to achieve a particular correspondence between input-output pairs. But early works in the field have shown that the number of threshold functions (or linearly separable functions) in N binary variables is small compared to the number of all possible boolean mappings in N variables, especially if N is large. This problem is one of the main limitations of most neural networks models where the state is fully specified by the environment during learning: they can only learn linearly separable functions of their inputs. Moreover, a learning procedure which requires the outside world to specify the state of every neuron during the learning session can hardly be considered as a general learning rule because in real-world conditions, only a partial information on the ``ideal'' network state for each task is available from the environment. It is possible to use a set of so-called ``hidden units'' [Hinton,Sejnowski,Ackley. 84], without direct interaction with the environment, which can compute intermediate predicates. Unfortunately, the global response depends on the output of a particular hidden unit in a highly non-linear way, moreover the nature of this dependence is influenced by the states of the other cells.",
isbn="978-3-642-82657-3"
}

% good intro to neural nets
@ARTICLE{LeCun1998,  
  author={Y. {Lecun} and L. {Bottou} and Y. {Bengio} and P. {Haffner}},  
  journal={Proceedings of the IEEE},   
  title={Gradient-based learning applied to document recognition},  
  year={1998},  
  volume={86},  
  number={11},  
  pages={2278-2324},  
  doi={10.1109/5.726791}
}

% mfcc basic
@ARTICLE{Mermelstein1980,
  author={Davis, S. and Mermelstein, P.},
  journal={IEEE Transactions on Acoustics, Speech, and Signal Processing}, 
  title={Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences}, 
  year={1980},
  volume={28},
  number={4},
  pages={357-366},
  doi={10.1109/TASSP.1980.1163420}
}

% statistical learning
@book{Vapnik1995,
  author = {Vapnik, Vladimir N.},
  title = {The Nature of Statistical Learning Theory},
  year = {1995},
  isbn = {0387945598},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg}
}

% svm
@article{Cortes1995,
  author = {Cortes, Corinna and Vapnik, Vladimir},
  title = {Support-Vector Networks},
  year = {1995},
  issue_date = {Sept. 1995},
  publisher = {Kluwer Academic Publishers},
  address = {USA},
  volume = {20},
  number = {3},
  issn = {0885-6125},
  url = {https://doi.org/10.1023/A:1022627411411},
  doi = {10.1023/A:1022627411411},
  abstract = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
  journal = {Mach. Learn.},
  month = sep,
  pages = {273–297},
  numpages = {25},
  keywords = {efficient learning algorithms, radial basis function classifiers, polynomial classifiers, pattern recognition, neural networks}
}

% deep learning imagenet
@inproceedings{Krizhevsky2012,
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  title = {ImageNet Classification with Deep Convolutional Neural Networks},
  year = {2012},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
  pages = {1097–1105},
  numpages = {9},
  location = {Lake Tahoe, Nevada},
  series = {NIPS'12}
}

% rnn
@article{Staudenmeyer2019,
  author    = {Ralf C. Staudemeyer and Eric Rothstein Morris},
  title     = {Understanding {LSTM} - a tutorial into Long Short-Term Memory Recurrent Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1909.09586},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.09586},
  archivePrefix = {arXiv},
  eprint    = {1909.09586},
  timestamp = {Tue, 24 Sep 2019 11:33:51 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-09586.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% --
% convolutional neural networks

% first paper about cnns
@inbook{LeCun1989_Generalization,
  title = "Generalization and network design strategies",
  author = "Yann Lecun",
  year = "1989",
  language = "English (US)",
  editor = "R. Pfeifer and Z. Schreter and F. Fogelman and L. Steels",
  booktitle = "Connectionism in perspective",
  publisher = "Elsevier"
}



% --
% dataset

% speech commands set
@article{Warden2018,
  title={Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition},
  author={Pete Warden},
  journal={ArXiv},
  year={2018},
  volume={abs/1804.03209}
}


% --
% benchmark networks

% list
https://paperswithcode.com/sota/keyword-spotting-on-google-speech-commands

% benchmark list
@online{PaperswithcodeKWS, 
    author = {Papers with Code},
    title = {keyword-spotting-on-google-speech-commands},
    year = 2021,
    url = {https://paperswithcode.com/sota/keyword-spotting-on-google-speech-commands},
    urldate = {2021-06-02}
}

% transformer network -> highes accuracy
@misc{Berg2021,
  title={Keyword Transformer: A Self-Attention Model for Keyword Spotting}, 
  author={Axel Berg and Mark O'Connor and Miguel Tairum Cruz},
  year={2021},
  eprint={2104.00769},
  archivePrefix={arXiv},
  primaryClass={eess.AS}
}

Wav2KWS: Transfer Learning from Speech Representationsfor Keyword Spotting


% --
% small-footprint or efficient

% sainath small footprint
@inproceedings{Sainath2015,
  title={Convolutional neural networks for small-footprint keyword spotting},
  author={Tara N. Sainath and Carolina Parada},
  booktitle={INTERSPEECH},
  year={2015}
}

% tu graz, resource efficient: https://arxiv.org/abs/2012.10138
@misc{Peter2020,
  title={Resource-efficient DNNs for Keyword Spotting using Neural Architecture Search and Quantization}, 
  author={David Peter and Wolfgang Roth and Franz Pernkopf},
  year={2020},
  eprint={2012.10138},
  archivePrefix={arXiv},
  primaryClass={eess.AS}
}

% deep residual
@article{Tang2017,
  author    = {Raphael Tang and
               Jimmy Lin},
  title     = {Deep Residual Learning for Small-Footprint Keyword Spotting},
  journal   = {CoRR},
  volume    = {abs/1710.10361},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.10361},
  archivePrefix = {arXiv},
  eprint    = {1710.10361},
  timestamp = {Mon, 13 Aug 2018 16:46:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1710-10361.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% power consumption
@article{Tang2018,
  author    = {Raphael Tang and
               Weijie Wang and
               Zhucheng Tu and
               Jimmy Lin},
  title     = {An Experimental Analysis of the Power Consumption of Convolutional
               Neural Networks for Keyword Spotting},
  journal   = {CoRR},
  volume    = {abs/1711.00333},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.00333},
  archivePrefix = {arXiv},
  eprint    = {1711.00333},
  timestamp = {Mon, 13 Aug 2018 16:46:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-00333.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% --
% adversarial, gan

% adv paper I
@misc{Goodfellow2014,
  title={Generative Adversarial Networks}, 
  author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
  year={2014},
  eprint={1406.2661},
  archivePrefix={arXiv},
  primaryClass={stat.ML}
}

% interesting
SELF-ATTENTION GENERATIVE ADVERSARIAL NETWORKFOR SPEECH ENHANCEMENt


% --
% wavenets

% original wavenet paper oord
@article{Oord2016,
  author    = {A{\"{a}}ron van den Oord and
               Sander Dieleman and
               Heiga Zen and
               Karen Simonyan and
               Oriol Vinyals and
               Alex Graves and
               Nal Kalchbrenner and
               Andrew W. Senior and
               Koray Kavukcuoglu},
  title     = {WaveNet: {A} Generative Model for Raw Audio},
  journal   = {CoRR},
  volume    = {abs/1609.03499},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.03499},
  archivePrefix = {arXiv},
  eprint    = {1609.03499},
  timestamp = {Mon, 13 Aug 2018 16:49:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/OordDZSVGKSK16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% fast wavenet
https://github.com/tomlepaine/fast-wavenet

% pytorch implementation example
https://github.com/vincentherrmann/pytorch-wavenet


% --
% transfer learning

% info website:
% https://cs231n.github.io/transfer-learning/


% --
% resnets
%https://pytorch-tutorial.readthedocs.io/en/latest/tutorial/chapter03_intermediate/3_2_2_cnn_resnet_cifar10/
% --
% theory

\section{Theory}\label{sec:nn_theory}
\thesisStateRevised
\thesisStateNew
The theory provided in this section, merely focuses on the used neural network architectures and some basic theory about neural networks.
The most basic element in neural networks is denoted as node, an abstract element, that defines input and output connections from and to other nodes within the network.
The edges of the connections from and to a node represents a multiplication with a scalar value, denoted as weight.
Each node usually incorporates an additive term, denoted as bias term.
All weights and bias terms are forming the parameters of a network and can be trained through backpropagation.
The output of a node is a scalar computed from all inputs and mapped with a non-linear function denoted as activation function.
A neural network can consist of thousand of nodes in each possible constellation of connections.
The structure of a neural network is defined by its layers, where a layer is a set of nodes with specific connection properties, that receive inputs from the previous layer and output connections to the next layer.
For example, a neural network consists of one convolutional layer followed by three fully-connected layers.
The last layer of a neural network usually represents the class labels of a classification tasks.
A loss function computes the difference between the predicted and the actual class label during training and is essential for the backpropagation algorithm, that updates each parameter in the network through the gradients of the obtained error.


% --
% activation functions

\subsection{Activation Functions}\label{sec:nn_theory_acti}
Activation functions for neural networks of a node in a current layer, are non-linear functions that usually maps the sum of the weighted inputs from nodes in a previous layer to a single output value $z \in \R$ as following:
\begin{equation}\label{eq:nn_theory_acti}
  z = h(\bm{w}^T \bm{x})
\end{equation}
where $h$ is the activation function, $\bm{w} \in \R^n$ is a weight vector and $\bm{x} \in \R^n$ an input vector for one specific node receiving connections from a total number of $n$ nodes in the previous layer.
The output of each node in a current layer is further connected to other nodes in the next layer and builds up the neural network.
The constraint of an activation function is, that an easy computable derivative of this function must exist in order to backpropagate its gradients.

The most famous activation function nowadays is the Rectified Linear Unit (ReLU) \cite{Zeiler2013_relu} function:
\begin{equation}\label{eq:nn_theory_relu}
  h(a) = \max{(0, a)}
\end{equation}
with $a \in \R$ as input to the activation function.
The big advantage of the ReLU function is, that its sub-gradients are very easy and fast to compute.
Further two other well known activation functions, that are used in Wavenets for example, are the sigmoid function:
\begin{equation}\label{eq:nn_theory_sigmoid}
  h(a) = \frac{1}{1 + \exp{-a}}
\end{equation}
and the tanh function:
\begin{equation}\label{eq:nn_theory_tanh}
  h(a) = \frac{\exp{a} - \exp{-a}}{\exp{a} + \exp{-a}}
\end{equation}
both are squeezing the signal between either $[0, 1]$ for the sigmoid and $[-1, 1]$ for the tanh activation function.
The mentioned activation functions are shown in \rfig{nn_theory_activation}.
\begin{figure}[!ht]
  \centering
    \subfigure[relu]{\includegraphics[width=0.3\textwidth]{./4_nn/figs/nn_theory_activation_relu}}
    \subfigure[sigmoid]{\includegraphics[width=0.3\textwidth]{./4_nn/figs/nn_theory_activation_sigmoid}}
    \subfigure[tanh]{\includegraphics[width=0.3\textwidth]{./4_nn/figs/nn_theory_activation_tanh}}
  \caption{Different activation functions for neural networks.}
  \label{fig:nn_theory_activation}
\end{figure}
\FloatBarrier
\noindent


% --
% fully-connected

\subsection{Fully Connected Layer}
A fully-connected (FC) layer is one of the simplest and most commonly used layer types in neural network architectures.
Each node from a previous layer is forwardly connected to all nodes in a current FC layer and each node in that current FC layer is connected to all nodes in the next layer.
Further every connection consist of one trainable weight and each node incorporates an additive bias term.
A simple FC layer is illustrated in \rfig{nn_theory_fc} with 3 nodes.
% fc
\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.30\textwidth]{./4_nn/figs/nn_theory_fc.eps}
  \caption{Basic fully-connected layer with 3 nodes, receiving connections from 2 input nodes and outputting connections to 2 output nodes.}
  \label{fig:nn_theory_fc}
\end{figure}
\FloatBarrier
\noindent
Mathematically one node $i$ performs following calculation:
\begin{equation}
  z_i = h(\bm{w_i}^T \bm{x} + b_i)
\end{equation}
with the same notations as described in \req{nn_theory_acti} and an additional bias term $b_i \in \R$.
For all nodes in a current layer this equation of a single node can be summarized to a weight matrix $W \in \R^{m \times n}$ with $m$ as total amount of nodes in the current layer and a total number of $n$ nodes from the previous layer, so that an FC layer can be written as:
\begin{equation}
  \bm{z} = h(W \bm{x} + \bm{b})
\end{equation}
where $\bm{b} \in \R^m$ are the bias terms and $\bm{z} \in \R^m$ the outputs of all nodes in the current layer $m$.
With the matrix representation it is easy to obtain the number of training parameters and amount of operations needed for a FC layer.
The amount of parameters are clearly $m \cdot n$ for $W$ and $m$ for $\bm{b}$, so in total this gives $m \cdot n + m$ parameters.
The amount of operations are the matrix vector multiplication with approximated amounts of multiplication and additions, plus the additive bias terms.
With the activation function disregarded, the amount of calculations are closely:
\begin{equation} 
  \mathcal{T}(W \bm{x} + \bm{b}) = 2 (m \cdot n) + m
\end{equation}
which would give for example with $n = 128$ and $m = 64$ roughly $\mathcal{T}(W \bm{x} + \bm{b}) = \SI{16.4}{\kilo\ops}$.


% --
% cnn

\subsection{Convolutional Layers}\label{sec:nn_theory_cnn}
Convolutional layers are the fundamental building block of every CNN, as already discussed in \rsec{prev_nn_cnn}.
Convolutional filters are applied on small areas of the input data to retrieve spatial information, where the input data can consist of multiple channels or input maps.
Those convolutional filters are also called kernels, denoted as $k$ and in case of 2 dimensional input maps illustrated as images with kernel width $d_{k_w}$ and height $d_{k_h}$.
The kernels are shifted over each input map in each dimensional axis with an operation called \emph{stride}, denoted as $s$, and produce an output map corresponding to each kernel and input map through the convolution operation, mathematically denoted with a star $\ast$.
The output length $d_{o_z}$ of an image in any dimension $z$, where $z \in \{w, h\}$ with $w$ as width and $h$ as height for images, convoluted from an input map of length $d_{x_z}$ with a kernel $k$ striding along axis $z$ with $s_z$ and kernel size for that axis $d_{k_z}$ can be computed as following:
\begin{equation}\label{eq:nn_theory_cnn_}
  d_{o_z} = \floor*{\frac{d_{x_z} + p_z - d_{k_z}}{s_z} + 1}
\end{equation}
where $p_z$ is an additional \emph{padding} term in dimension $z$, for instance zero-padding adds zeros on both sides of the selected axis.
For example if a $16 \times 16$ image is convoluted by a $5 \times 5$ kernel with stride $1$ in each direction and no padding is done, the output image is a $12 \times 12$ image.
The padding operation has usually the purpose to keep the same output and input dimension.
For instance, this is necessary in residual neural networks, where the input to a residual block with several convolutional layers is bypassed and added to the output of the same residual block again.
Being able to compute the addition operation from input and output of the residual block, their dimensions must coincide.
However in most convolutional network applications without residual blocks, it is preferred not to pad the image, so that dimensions are reduced hence parameters and multiplications saved for further consecutive layers.
Further some special types of sub-sampling layers exist, such as a Max-Pooling layer, designed to reduce the dimensions of the output image.

A convolutional layer is defined by the amount of input maps and output channels (feature maps), the kernel size, the stride of the kernel and some other specialties like padding and dilation.
However it is not immediately clear from those parameter, how many convolutional filters are applied and how the output feature maps are calculated exactly.
The number of convolutional filters $\#k$ is in most practical examples always:
\begin{equation}\label{eq:nn_theory_n_filters}
  \#k = i \cdot j
\end{equation}
where $i$ and $j$ is the amount of input and output channels respectively.
Each kernel produces an output map, but the idea is to constraint the number of output maps $\#k$ to the defined output channels $j$.
This is usually done by summing up all feature maps obtained from all input channels $i$ convoluted with kernel $k_{i, j}$ for one output channel $j$:
\begin{equation}
  o_j = \sum_{i} k_{i, j} \ast x_i
\end{equation}
where $o_j$ is the resulting j-th feature map or output channel and $x_i$ the i-th input channel.
A graphical example of this procedure is shown in \rfig{nn_theory_cnn_basics}.
% cnn basics
\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.6\textwidth]{./4_nn/figs/nn_theory_cnn_basics.eps}
  \caption{Basic convolutional layer with a $16 \times 16$ input image, decomposed in 3 channels (CYM) and processed to 4 output feature maps. The kernel size is $5 \times 5$ and the stride is $1$ in both dimensions.}
  \label{fig:nn_theory_cnn_basics}
\end{figure}
\FloatBarrier
\noindent

The amount of parameters are all weights in the convolutional filters (kernels) and their bias terms, however the bias terms for each filter can be ignored, in this thesis no bias terms were used for the convolutional layers.
Therefore the amount of parameters are $\#k \cdot d_{k_w} \cdot d_{k_h}$ for a two dimensional convolutional layer, which makes $3 \cdot 4 \cdot 5 \cdot 5 = 300$ parameters for the example in \rfig{nn_theory_cnn_basics}.
The amount of operations are all convolution operations for each filter and the summation to $j$ output channels.
One convolution with a single two dimensional kernel $k$ and one input channel $x$ has an approximate number of multiplication and addition operations:
\begin{equation}
  \mathcal{T}(k * x) = 2(d_{k_w} \cdot d_{k_h}) (d_{o_w} \cdot d_{o_h})
\end{equation}
and for all feature maps and the summation of them to the output channels:
\begin{equation}
  \mathcal{T}(o) = \#k \cdot \mathcal{T}(k * x) + \#k \cdot (d_{o_w} \cdot d_{o_h})
\end{equation}
For the previous example this would give $\mathcal{T}(k * x) = 2 (12 \cdot 12) (5 \cdot 5) = \SI{7.2}{\kilo\ops}$ and $\mathcal{T}(o) = 12 \cdot \SI{7.2}{\kilo\ops} + 12 \cdot (12 \cdot 12) = \SI{88.1}{\kilo\ops}$.


% --
% loss functions

\subsection{Loss Functions and Softmax}
Loss functions also known as cost functions, are used to determine the difference between the predicted label $\hat{y}_i$ for class $i$ compared to the actual or ground truth label $y$ of one specific training data example.
The predicted labels are usually presented by the output nodes in the last layer of a neural network.
Therefore $\hat{\bm{y}} = [\hat{y_0}, \hat{y_1}, \dots, \hat{y_L}]^T$ has the dimension of the number of labels or classes $L$.
Often it is preferred that $\hat{\bm{y}} \in \R^L$ provides a probability distribution such that:
\begin{equation}
  \sum_{i=0}^L \hat{y}_i = 1
\end{equation}
which can be achieved with the softmax function:
\begin{equation}\label{eq:nn_theory_softmax}
  \hat{y}_i = \frac{\exp{x_i}}{\sum_{j=0}^{L}\exp{x_j}}
\end{equation}
where $L$ is the amount of nodes in this layer, if it is the last layer as usual for the softmax, $L$ is the total amount of class labels and $\hat{y}_i$ the probability value of the corresponding class $i$.

There are already plenty of different loss functions available for training neural networks, such as described in \cite{LeCun2006}, however in this thesis only one kind of it is used for all neural network architecture, the cross-entropy loss or for two classes the binary cross-entropy loss function.
Without going too much into the detail of this loss function, it is described in the same way as in the \texttt{Pytorch} framework \cite{Pytorch}.
The binary cross-entropy is implemented as:
\begin{equation}\label{eq:nn_theory_binary_cross_entropy}
  l(x_i, y_i) = y_i \cdot \log x_i + (1 - y_i) \cdot \log (1 - x_i)
\end{equation}
with the training pair $(x_i, y_i)$, where $x_i \in \R^{N \times 1}$ as feature input and $y_i \in \{0, 1\}^N$ as corresponding label for the two label classification problem.
During the training procedure a whole batch of sample size $N$ is used for one update step, however the loss function should provide only a single value, therefore all $l_i$ with $i = 0, \dots, N$ must be concatenated.
This can simply be done for instance with the calculation of the mean of all $l_i$.

The multiclass variant of the cross-entropy can be defined by
\begin{equation}
  l(x_i, y_i) = - x_i[y_i] + \log{\left( \sum_{j=0}^{L} \exp{x_i[j]} \right)}
\end{equation}
where $x_i \in \R^{N \times L}$, $y_i \in \{0, 1, \dots, L\}^N$ and a total number of $L$ classes.
Note that $x_i[y_i]$ is denoted as indexing of $x_i$ of its class dimension $L$. 


% --
% dropout

\subsection{Dropout}
Dropout \cite{Hinton2012} is a method to improve generalization and training of neural networks.
The idea is to set the output of randomly selected nodes within a layer for one training step to zero, so that only the other nodes are updated.
This can be done by multiplying all outputs of a current layer with a vector containing a specified number of zeros and ones places at random positions within the vector.
The amount of zeros compared to ones can be sampled from a Bernoulli distribution set by a probability value, for instance $p=0.2$ means that there are \SI{20}{\percent} zeros and \SI{80}{\percent} ones randomly placed within the vector.


% --
% training

\subsection{Training of a Neural Network}
The training of a neural network is usually done by updating each parameter of the model with back-propagated gradients from the loss obtained at the output nodes.
The loss is calculated by a specific loss function comparing the predicted labels to the targets of the actual training samples.
To update the parameters of the network, an update rule, such as Stochastic Gradient Descend (SGD) or Adam \cite{Kingma2015} is applied.
Note that a single update step is usually not performed on the whole dataset, unless it is a very small dataset. 
In the normal case the update is performed on small chunks of the dataset called \emph{batch}.
One training iteration is therefore the update of parameters for a single batch.
The run over all batches of a dataset is defined as \emph{epoch}.
Note that the training of neural networks may take over thousands of epochs until convergence is reached.
The general training techniques for neural networks and further details are not described here, because of many existing books and papers \cite{LeCun2006}, \cite{Goodfellow2016}, \cite{DeepLearning} that are already presenting this topic very well. 
Further it does not add any value to this thesis as the algorithms are well examined and run in the background of all neural network frameworks.
The interesting elements are therefore only the neural network architecture design, the hyperparameters for training and the used loss functions.


% --
% GANs

\subsection{Generative Adversarial Neural Networks Game}\label{sec:nn_theory_gan}
\thesisStateNew
GANs \cite{Goodfellow2014}, as already mentioned in \rsec{prev_nn_adv} are deploying two neural networks: a Generator (G) network for generating fake images and a Discriminator (D) network for discriminating between fake and real images.
The min-max game both networks are playing is the following:
\begin{equation}\label{eq:nn_theory_gan}
  \underset{G}{\min} \, \underset{D}{\max} \, V(D, G) = \E_{\bm{x} \sim p_{data}(\bm{x})}\left[ \log D(\bm{x}) \right] + 
    \E_{\bm{z} \sim p_{\bm{z}}(\bm{z})}\left[ \log (1 - D(G(\bm{z}))) \right]
\end{equation}
where $\bm{x} \in \mathcal{X}$ are samples from the training data $p_{data}$, $\bm{z} \in \mathcal{Z}$ is a latent variable sampled from $p_{\bm{z}}$, $D: \mathcal{X} \mapsto [0, 1]$ is the discriminator network and $G: \mathcal{Z} \mapsto \mathcal{X}$ the generator network.
The advise from the original paper is not to minimize G with $\log (1 - D(G(\bm{z})))$, but to maximize $\log D(G(\bm{z}))$ instead.
The training algorithm can be read from \cite{Goodfellow2014} in detail.
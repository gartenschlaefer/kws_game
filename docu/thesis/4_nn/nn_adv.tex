% --
% adversarial

\section{Adversarial Pre-Training}\label{sec:nn_adv}
In adversarial neural network training, two separate neural networks are competing against each other in an adversary task.
This competition of the two networks motivates them to improve their performance and beat the other network.
The application of GANs, as already explained in \rsec{prev_nn_adv} and \rsec{nn_theory_gan}, is an interesting subject in research and the questions whether the obtained weights from the separate training of the Generator and Discriminator network can contribute to better the performances in equivalent models, arises.

This thesis evaluates the transfer of trained filter weights of the convolutional layers from either G or D to initialize an equivalent CNN by using two different pre-training methods.
As already mentioned in \rsec{prev_nn_adv}, the convolutional layer structure, such as the filter size and striding properties, of G and D has to be the same as in the target CNN, however, the amounts of filters in those layers can vary.
One method, denoted as \emph{adversarial dual train}, is to let G and D have the same amount of filters as the target CNN and to consider all available class labels in the vocabulary for training.
The other method, referred to as \emph{adversarial label train}, applies several separate training instances of G and D on small subsets of class labels, where G and D gets fewer amounts of filters compared to the target CNN.
Those methods are abbreviated in some experiments as \enquote{adv-dual} and \enquote{adv-label} for the adversarial dual and adversarial label train, respectively.
Only the adversarial label train is explained in detail because the adversarial dual train is straight forward once the adversarial label train is understood.
Note that the adversarial pre-training methods perform only on the \texttt{adv-d-jim}, \texttt{adv-g-jim}, and \texttt{conv-jim} models, which are described in \rsec{nn_arch}.


% --
% training GANs

\subsection{Training Generative Adversarial Neural Networks}\label{sec:nn_adv_train}
The interesting part in training GANs is how the G and D models are updated in each training step and which loss functions are applied.
In \req{nn_theory_gan} the game is notated as min-max game, from which the loss of D, referred to as $l_D$, for one specific training example $i$ is
\begin{equation}
  l_D(x_i, z_i, G) = l(D(x_i), y_r) + l(D(G(z_i)), y_f),
\end{equation}
where $l$ is the binary cross-entropy loss, as described in \req{nn_theory_binary_cross_entropy}, $D: \mathcal{X} \mapsto [0, 1]$ and $G: \mathcal{Z} \mapsto \mathcal{X}$ are the D and G model, $x_i \in \mathcal{X}$ is a data example, $z_i \in \mathcal{Z}$ is a randomly sampled latent variable, $y_r = 1$ is the real label, and $y_f = 0$ the fake label.
In contrast the loss of G, denoted as $l_G$, is given by
\begin{equation}
  l_G(z_i, D) =  l(D(G(z_i)), y_r),
\end{equation}
which is rewritten in order to perform a maximization of $\log D(G(\bm{z}))$, as noted in \rsec{nn_theory_gan}.

An extended approach so that G produces samples specifically similar to the data distribution and does not drift off into creating unrealistic fakes of noisy samples to fake D, is to add a similarity term such as the \emph{cosine similarity} defined by
\begin{equation}
  s(\bm{x_1}, \bm{x_2}) = \frac{\bm{x_1}^T \bm{x_2}}{\norm{\bm{x_1}}_2 \cdot \norm{\bm{x_2}}_2 + \epsilon},
\end{equation}
where $s : (\mathcal{X}, \mathcal{X}) \mapsto [0, 1]$ is the cosine similarity function, $\bm{x_1}$ and $\bm{x_2}$ are two vectors for similarity measure, and $\epsilon$ is a small number such that no division by zero is possible.
With the additional similarity loss, $l_G$ gets
\begin{equation}
  l_G(x_i, z_i, D) =  l(D(G(z_i)), y_r) + \lambda \left(1 - \frac{1}{C} \sum_{c=0}^{C} s(\hat{\bm{e}}_c^T x_i , \hat{\bm{e}}_c^T G(z_i)) \right),
\end{equation}
where $\hat{\bm{e}}_c \in \{1, 0\}^C$ is an unit vector representing one cepstral coefficient of the Mel Frequency Cepstral Coefficient (MFCC) data $x_i \in \mathcal{X} = \R^{C \times M}$ with a total number of $C$ MFCC coefficients and $M$ frames.
Further, $\lambda$ is a trade-off factor between data similarity and fake loss from D.
Note that the experiments in \rsec{exp_adv} choose $\lambda = 5$.

The update of D and G is performed in each training step by backpropagating the obtained losses.
However, it is more appealing to update D for a certain numbers of training steps with no update of G and then switching to updates of G without updating D.
This will give either D or G some update steps to improve in their specific adversarial task of either discriminating or generating.
In this thesis, the training steps for updating either D or G are 2 epochs for each model.
Note that an epoch consists of several training steps depending on the batch size and amount of data.
This can vary in the experiments but does not influence the overall end results dramatically.


% --
% label train

\subsection{Adversarial Label Train}
Adversarial label train is the transfer of convolutional filter weights that are collected from multiple GAN training instances on subsets of all available class labels in the vocabulary.
For instance, if the label set in the vocabulary is \{\enquote{left}, \enquote{right}\}, then a separate training instance may focus only on the label \enquote{left} and another on the label \enquote{right}.
It is important to assign a specific number of filters to each label train instance.
For example, each label train gets 8 filters in the first convolutional layer.
\rfig{nn_adv_label_scheme} illustrates the label train scheme through 6 label train instances, such as used in the experiments in \rsec{exp_adv}.
\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.85\textwidth]{./4_nn/figs/nn_adv_label_scheme.pdf}
  \caption{Adversarial label train scheme showing the first convolutional layers of \texttt{adv-d-jim} and \texttt{adv-g-jim} with 6 training instances of label subsets using 100 epochs. The learned filters are intended to initializes the \texttt{conv-jim} model.}
  \label{fig:nn_adv_label_scheme}
\end{figure}
\FloatBarrier
\noindent
To further investigate the adversarial label train, the training results of the models \texttt{adv-d-jim} and \texttt{adv-g-jim} are provided in the following.
\rfig{nn_adv_loss_label} shows the training loss of an actual GAN training for the label subset \{\enquote{left}, \enquote{go}\}.
\begin{figure}[!ht]
  \centering
  \subfigure[100 epochs]{\includegraphics[width=0.48\textwidth]{./4_nn/figs/nn_adv_loss_label_it-100.png}}
  \quad
  \subfigure[1000 epochs]{\includegraphics[width=0.48\textwidth]{./4_nn/figs/nn_adv_loss_label_it-1000.png}}
  \caption{Adversarial training loss of the label subset \{\enquote{left}, \enquote{go}\}.}
  \label{fig:nn_adv_loss_label}
\end{figure}
\FloatBarrier
\noindent
Note that the update of either D or G performs alternately every 2 training epochs, as described in \rsec{nn_adv_train}.
\rfig{nn_adv_fakes_label} shows the generation of fake images from G with different amounts of training epochs.
\begin{figure}[!ht]
  \centering
  \subfigure[100 epochs]{\includegraphics[width=0.48\textwidth]{./4_nn/figs/nn_adv_fakes_label_it-100.png}}
  \quad
  \subfigure[1000 epochs]{\includegraphics[width=0.48\textwidth]{./4_nn/figs/nn_adv_fakes_label_it-1000.png}}
  \caption{Generation of fake images of the label subset \{\enquote{left}, \enquote{go}\} with different amounts of epochs.}
  \label{fig:nn_adv_fakes_label}
\end{figure}
\FloatBarrier
\noindent
\rfig{nn_adv_label_weights_d} and \rfig{nn_adv_label_weights_g} show trained weights of the filters in the first convolutional layer of D and G, respectively.
Those were obtained from an adversarial label train with different amount of epochs.
\begin{figure}[!ht]
  \centering
  \subfigure[D with 100 epochs]{\includegraphics[width=0.48\textwidth]{./4_nn/figs/nn_adv_label_weights_d-100.png}}
  \quad
  \subfigure[D with 1000 epochs]{\includegraphics[width=0.48\textwidth]{./4_nn/figs/nn_adv_label_weights_d-1000.png}}
  \caption{Concatenated label weights of the filters in the first convolutional layer from the Discriminator model with different amounts of epochs.}
  \label{fig:nn_adv_label_weights_d}
\end{figure}
\FloatBarrier
\noindent
\begin{figure}[!ht]
  \centering
  \subfigure[G with 100 epochs]{\includegraphics[width=0.48\textwidth]{./4_nn/figs/nn_adv_label_weights_g-100.png}}
  \quad
  \subfigure[G with 1000 epochs]{\includegraphics[width=0.48\textwidth]{./4_nn/figs/nn_adv_label_weights_g-1000.png}}
  \caption{Concatenated label weights of the filters in the first convolutional layer from the Generator model with different amounts of epochs.}
  \label{fig:nn_adv_label_weights_g}
\end{figure}
\FloatBarrier
\noindent
The amount of training epochs is essential because it determines how much the models learn during their adversarial training.
With 100 training epochs, G already creates similar convolutional filters for each label train instance because it does not need to be that accurate in creating different looking fakes.
Yet D improves as well and the need of generating different looking fakes motivates G to match up.
By using 1000 training epochs, G generates many different looking fakes, as already shown in \rfig{nn_adv_fakes_label}, and the filters take on various structures.
Note that a second convolutional layer for the \texttt{adv-d-jim} and \texttt{adv-g-jim} exist as well.
\rfig{nn_adv_label_weights_conv1} shows their corresponding weights for 100 training epochs.
\begin{figure}[!ht]
  \centering
  \subfigure[D with 100 epochs]{\includegraphics[width=0.25\textwidth]{./4_nn/figs/nn_adv_label_weights_conv1_d-100.png}}
  \qquad \qquad
  \subfigure[G with 100 epochs]{\includegraphics[width=0.25\textwidth]{./4_nn/figs/nn_adv_label_weights_conv1_g-100.png}}
  \caption{Concatenated label weights of the filters in the second convolutional layer from the Discriminator and Generator model trained with 100 epochs.}
  \label{fig:nn_adv_label_weights_conv1}
\end{figure}
\FloatBarrier
\noindent
Each row of the second convolutional layer corresponds to a single feature map produced by the first convolutional layer and therefore 8 rows correspond to one adversarial label training instance.
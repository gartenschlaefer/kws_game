% --
% prev keyword spotting

\section{Work on Keyword Spotting with Neural Networks}\label{sec:prev_kws}
The presented work on KWS with neural networks were selected to either computationally efficient solutions or benchmark networks for the speech commands dataset \cite{Warden2018}.


% --
% energy efficient

\subsection{Energy Efficient Solutions}
One famous paper on low computational footprint of CNNs regarding KWS tasks, was presented by Sainath et al. \cite{Sainath2015}.
Two neural network architectures were chosen from this paper, one is a traditional CNN and the other a limited multipliers CNN striding only in the frequency axis.
Both networks are described in detail in \rsec{nn_arch}.

The deployment of a KWS system on microcontrollers was examined in \cite{Zhang2017}.
Different neural network architectures were evaluated regarding their memory usage and operations per inference.
The work showed that 32-bit to 8-bit quantization does not reduce the classification accuracy and that the depth-wise separable CNN (DS-CNN) was the best choice regarding limited amounts of memory and operations in comparison to the other evaluated models.

Another highly optimized resource efficient Deep Neural Network (DNN) based on CNNs with parameters obtained by a Neural Architecture Search (NAS), is described in \cite{Peter2020}.
This work also showed that a 1-bit quantization of the weights perform nearly as well as the 8-bit quantization, which decreases the memory usage even further.


% --
% benchmark

\subsection{Benchmark Networks for this Thesis}\label{sec:prev_kws_benchmark}
First it is to mention that the speech commands dataset \cite{Warden2018} exists in two versions (\texttt{v0.01} and \texttt{v0.02}) consisting of raw audio data in the \texttt{.wav} format.
Furthermore, there is no pre-processing or feature extraction done beforehand.
Also it is up to the users to select the labels for the vocabulary in a KWS task.
However, the intended concept is to choose the so-called \emph{core keywords} as classification labels and add an \enquote{unknown} label for the \emph{auxiliary keywords}.
Also a separate \enquote{noise} or \enquote{silence} label can be added from given noise data files.
In most papers the core keywords were \{\enquote{left},  \enquote{right}, \enquote{up}, \enquote{down}, \enquote{go}, \enquote{stop}, \enquote{yes}, \enquote{no}, \enquote{on}, \enquote{off}\}.
Further, two labels for \enquote{silence} and \enquote{unknown} were usually added.
The collection of core keywords and the two additional labels is referred to as \texttt{L12}.
More details about the dataset is presented in \rsec{exp_dataset}.

One benchmark is represented by the work of \cite{Berg2021} introducing a Keyword Transformer (KWT) network.
A quick overview of continually updated benchmark scores regarding the speech commands dataset is provided in \cite{PaperswithcodeKWS}.
All benchmark scores from the referenced work in this section are shown in \rtab{prev_kws_bench}.
\input{./2_prev/tables/tab_prev_kws_bench.tex}
Note that these scores are achieved by sophisticated neural network architectures consisting of many layers and that appraoching these scores is usually very hard.
% --
% prev neural networks basics

\section{History and Work on Neural Networks}\label{sec:prev_nn}
Historical remarks on neural networks are presented as well as a brief overview upon some basic and modern architectures that are meaningful regarding this thesis.


% --
% prev history

\subsection{Historical Remarks on Neural Networks}\label{sec:prev_nn_history}
The first step towards computational neural networks, as they are known today, was the introduction of the so called \enquote{Perceptron} by Rosenblatt in the year 1958 \cite{Rosenblatt1958}. 
The idea of the Perceptron emerged from physiologists, trying to model a physiological neural network in computational terms. 
This first model was based on the information processing of the retina (input nodes), which passes through several physiological neural networks (hidden nodes) and finally elicit an action or decision (output nodes).
Publishing his work and implementing those ideas in an actual computer system (at those time computers were huge boxes), Rosenblatt kicked of the domain of computational learning systems.
The race of finding best neural network architectures for specific regression or classification tasks had begun.

Another great advance in the history of neural networks, was the introduction of a very famous learning algorithm known as \enquote{backpropagation}, evolved by several authors at the same time \cite{LeCun1986} and \cite{Rumelhart1986} in the late 80s. 
Even nowadays, 35 years after introducing backpropagation, it is still the \emph{de facto} standard in training neural networks.
Nowadays backpropagation is implemented in every machine learning framework for neural networks as its core element.
Such frameworks are, for instance, \texttt{Pytorch} \cite{Pytorch} or \texttt{Tensorflow} \cite{Tensorflow}, which are capable of handling the gradient calculation (automatic differentiation) and backpropagation algorithm of those gradients in the background.

The neural networks reputation during the time until now, was not always seen that splendid.
The general problem of handling overfitting (prevent networks from learning data samples by heart) and improve generalization on unseen data, is still an unsolved problem in many applications.
Some mathematicians working in the field of statistical methods in learning theory for pattern recognition, regard neural networks as being not meaningful in the advance of learning theory, such as one quote from \cite{Vapnik1995} of Vapnik's book in 1995 about natural learning theory states:
\begin{quote}
...In spite of important achievements in some specific applications using neural networks, the theoretical results obtained did not contribute much to general learning theory...
\end{quote}
This quote is true in the sense of that the complexity of neural networks consisting of many layers makes the tracing of the learning process very difficult.
No concrete formulas apart from the calculation of gradients exactly explains what neural networks are learning.
Therefore, many researchers remained using the classical statistical learning methods, such as the famous Support Vector Machines (SVM) \cite{Cortes1995}.
Over a long time, SVMs were preferred over neural networks because they were better understood due to their profound mathematical formulations and their achievements of state of the art performances by applying sophisticated feature extraction stages in between.
Not until 2012, neural networks gained more popularity again by scoring new benchmarks in image classification tasks with one famous paper \cite{Krizhevsky2012} where the previous benchmark of the best statistical method was bested by a significant score.
Deep Learning was the new key to success, where neural network architectures consist of many layers and large amounts of parameters to train.
Also in audio and language processing tasks, such as KWS, ASR and Natural Language Processing (NLP), the famous Hidden Markov Models (HMM) and other statistical methods get more and more replaced by neural networks.


% --
% convolutional nets

\subsection{Convolutional Neural Networks}\label{sec:prev_nn_cnn}
CNNs are a special type of neural networks that are consisting of so called convolutional layers.
Convolutional layers are able to incorporate spatial information from the input data through the application of convolutional filters.
Those convolutional filters create so called feature maps as outputs.
Spatial information is very important in images, where neighboring pixels are correlated to each other, such as edges or homogeneous regions within an image.
Convolutional filters are very commonly applied in image processing tasks, such as denoising and other enhancements.
A classical application of convolutional filters in audio processing, is a simple average filter applied on the signal's energy representation over time, to determine onsets such as the start of a speech signal.

Still it took remarkably long until convolutional filters were a common and a widely used asset in neural network architectures.
The general concepts of CNNs were examined by LeCun et al. on handwritten postal codes in 1989 \cite{LeCun1989_Generalization}.
Further research and experiments on the famous MNIST dataset of handwritten digits, were performed in the late 90s \cite{LeCun1998} and asserted the success of CNNs.
A classical convolutional layer in CNNs consists of multiple convolutional filters with trainable weights and an additive bias term (optional) per filter, followed by a non-linear activation function applied to the computed feature maps.
More details about CNNs and how they transform input images to feature maps is presented in \rsec{nn_theory_cnn}.


% --
% recurrent neural networks

\subsection{Recurrent Neural Networks}\label{sec:prev_nn_rnn}
Recurrent Neural Networks (RNN) are a type of neural networks that are using feedback loops from the output of each node back to its input.
With those feedback loops it is possible to accumulate information of sequential input data and therefore might present a solution to overcome the restriction of fixed sized input data.
Although RNNs are known to be hard to train because of the well known \emph{exploding or vanishing gradients} problem.
This problem can be solved by using so called Long Short Time Memory (LSTM) that are incorporating gates for the information storage in each cell. 
However, the LSTM cell also increases the amount of parameters to train and backpropagation over a large amount of time steps is computationally intensive.

Through their design to capture sequential data, RNNs are commonly applied in speech recognition tasks.
Note that RNNs are not subject of evaluation in this thesis due to their usually high computational footprint.
The interested reader is referred to the summary presented in \cite{Staudenmeyer2019} including further references upon the mentioned problems and architecture types.


% --
% wavenets

\subsection{Wavenets}\label{sec:prev_nn_wavenet}
Processing raw audio data as inputs to neural networks seemed to be difficult for a long time, mainly due to the huge amount of input dimensions of raw audio samples for even short time intervals.
Recently neural network architectures emerged with the ability to process raw audio samples.
One very prominent architecture, originally intended for natural speech generation, is the so called \emph{Wavenets} \cite{Oord2016}.

With the application of \emph{dilated convolution} and a quantization of the audio sample values, Wavenets can afford to process a huge amount of input dimensions and at the same time extract relevant information.
The implementation of Wavenets should be more efficient compared to LSTMs, such as the quote from \cite{Oord2016} states:
\begin{quote}
  ...With WaveNets we have shown that layers of dilated convolutions allow the receptive field to grow longer in a much cheaper way than using LSTM units...
\end{quote}
Note that research papers on audio classification tasks with Wavenets are rarely to be found.
Nevertheless, a classification task of musical pieces to corresponding music artists presented in \cite{Zhang2020}, was done with a Wavenet model.
Regarding this thesis, it is questionable whether Wavenets are suited for the KWS task of speech commands especially when considering only energy efficient solutions.


% --
% adversarial nets

\subsection{Generative Adversarial Neural Networks}\label{sec:prev_nn_adv}
GANs emerged from Goodfellow et al. in 2014 \cite{Goodfellow2014} and was initially motivated by Game Theory of a two player zero-sum game with the Min-Max-Theorem as solution concept \cite{VonNeumann1944}.
The game in terms of GANs is as follows: Two neural networks try to outperform the other in an adversary task, where the task of one network, denoted as Generator (G), is to counterfeit fake images, while the other network, denoted as Discriminator (D), has the task to discriminate, whether its been given real or fake images.
In \cite{Radford2016} some practical guidelines for designing and training deep GAN architectures are provided by applying the Deep Convolutional Generative Adversarial Network (DCGAN) model.
However, DCGANs merely consider image data and not speech signals but still were the first starting point for developing GANs in this thesis.

The G network, has to perform a so called \emph{transposed convolution}, \emph{up-convolution} or \emph{de-convolution} for upsampling the low-dimensional image to the desired output dimensions of real sample images from a dataset.
The transposed convolution and some problems regarding GANs are described in \cite{Durall2020}.

GANs are not the only application of adversarial neural networks, in \cite{Oezdenizci2020} it is shown how an adversarial network improves generalization over invariance of subject specific samples from Electroencephalogram (EEG) data.
The adversarial network in this paper is connected to a convolutional encoder network, which is also input to a classifier network.
The update of the encoder network is therefore done via the adversarial network and the classifier network at the same time and specified in a collective loss function.

In this thesis, the concept of GANs is used for pre-training weights to improve generalization and performance in a CNN network.
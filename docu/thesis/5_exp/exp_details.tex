% --
% training details

\section{Implementation and Experiment Details}\label{sec:exp_details}
\thesisStateReady
The implementation describes the software tools used for the experiments on the neural networks, such as the programming language and packages for the source code.
The training details provide commonly used sets of hyperparameters applied to train neural networks.
Note that the training details can vary between different experiments and the listing gives merely an overview of all available parameters to choose from and a recommendations of parameters for the speech command dataset.
If parameters are varying in some experiments, they are usually noted, otherwise the parameters listed were used.
The evaluation details of the models is described separately and evaluates the trained models upon accuracy and noise and shift invariance.


% --
% implementation notes

\subsection{Implementation Notes}\label{sec:exp_details_implementation}
The source code for this thesis was entirely written in \texttt{Python} with version $>3.8$ evaluated on a Linux operating system and is open source available in \cite{KWSGame}.
The operating system might be important if someone tries to run the python code on a Windows machine, where unexpected errors might occur (especially regarding path variables).
Also the project does not download the speech command dataset by its own and a path to the dataset has to be specified in the \texttt{config.yaml} file of the project.
More on information about how to run the project is described in the \texttt{README.txt} file.
The training and implementation of all used neural networks were done with the \texttt{Pytorch} \cite{Pytorch} framework of version $1.7.0$. 
Usually it should not be a problem if a newer version of \texttt{Pytorch} is used, during the work on the thesis the version changed to $1.9.0$ without any version depending problems.
The feature extraction of Mel Frequency Cepstral Coefficients (MFCC) was self implemented, but use already existing and efficient code for transforms, such as the Short Time Fourier Transform (STFT) or Discrete Cosine Transform (DCT), with packages from \texttt{Scipy}.
All matrix-vector computations were done with the well known package \texttt{Numpy} or with \texttt{Pytorch}.
Several other \texttt{Python} packages were used within the project, but are not named explicitly, they can be looked up in the open source repository of the project, if requested.


% --
% training details

\subsection{Neural Network Training Details}\label{sec:exp_details_training}
The training details of the used neural networks described in \rsec{nn_arch} can be split into following parameters:
\begin{enumerate}
  \item Feature extraction parameters
  \item Dataset parameters
  \item Training hyperparameters
  \item Pre-Training details
\end{enumerate}
The feature extraction parameters provide information about how the MFCC features were extracted in detail.
The dataset parameters are the selected labels and the number of examples per labels for training.
With the training hyperparameters, the neural network training is classically specified, such as learning rate or batch size.
The pre-training details are describing a separate training of for instance Generative Adversarial Neural Networks (GAN), to use the obtained weights for Convolutional Neural Networks (CNN).
Therefore the pre-training details are similar to the training hyperparameters and describe the training details of both the Generator (G) and Discriminator (D) network of the GANs.


% --
% feature

\subsubsection{Feature extraction parameters}
During the feature selection experiments in \rsec{exp_fs}, the parameters for cepstral coefficients with enhancements are varying.
If not other stated, the feature extraction parameters in \rtab{exp_details_params_feature} are used.
\input{./5_exp/tables/tab_exp_details_params_feature}


% --
% dataset

\subsubsection{Dataset parameters}
The selected labels for training are either the 12 labels (L12) for comparison to the benchmark networks described in \rsec{prev_kws_benchmark} or 7 labels (L7) used for the deployed KWS game are listed as well as the number of examples per label in \rtab{exp_details_params_dataset}.
\input{./5_exp/tables/tab_exp_details_params_dataset}
The maximal number of examples per label for training is chosen by the minimum examples per selected label for each label and each set.
This is provided by the validation set of \enquote{go} with \{train: 2948, test: 425, validation: 350 \} shown in \rtab{exp_dataset_all_labels}, with 350 examples per label in the validation set, considering a \SI{10}{\percent} split of both test and validation set.
This gives a maximal amount of 3500 examples to represent the whole dataset.
This is important, because the number of examples per label should not be chosen to higher values than 3500 for training, otherwise the same amount of examples per label is not given.


% --
% training hyperparameters

\subsubsection{Training hyperparameters}
The hyperparameters for training of the CNNs models described in \rsec{nn_arch} are shown in \rtab{exp_details_params_train}.
\input{./5_exp/tables/tab_exp_details_params_train}
From the experiments in \rsec{exp} it is shown, that epochs of 2000 yield into some small overfitting effects, but it was important to oberserve when these effects are present.
The batch size of 32 is slected to a low number, because it worked well and the amount of classes are at maximum 12.
Note that the batch size influences the selection of the learning rate for updating the parameters of the neural network models.


% --
% training parameters

\subsubsection{Pre-Training details}
The pre-training parameters describe the training of the Generative Adversarial Neural Networks (GAN), as shown in \rsec{nn_arch_adv} and \rsec{nn_adv}.
The hyperparameters shown in \rtab{exp_details_params_pre_train} are the same as for usual model training, but the Discriminator (D) and Generator (G) network can have different values for the hyperparameters.
\input{./5_exp/tables/tab_exp_details_params_pre_train}
It is important that the epochs for 


% --
% evaluation details

\subsection{Evaluation details}\label{sec:exp_details_tb}
The main evaluation score of the trained models is the computation of the accuracy on the test sets described in \rsec{exp_dataset}.
The accuracy is simply obtained by counting all correct classifications and dividing it by the number of classified samples $n$.
A score function $c(\hat{y}_i, y_i)$ for the accuracy can be defined as:
\begin{equation}
  c(\hat{y}_i, y_i) = 
  \begin{cases}
    1, & \text{if } \hat{y}_i = y_i\\
    0, & \text{otherwise} 
  \end{cases}
\end{equation}
where $\hat{y}_i \in \mathcal{L} = \{0, 1, \dots, L\} $ is the predicted label and $y_i \in \mathcal{L}$ the actual label of the sample $i$ with a total number of class labels $L$.
The accuracy $a \in [0, 1]$ can therefore be written as:
\begin{equation}
  a = \frac{1}{n} \sum_{i=0}^n c(\hat{y}_i, y_i)
\end{equation}
Another more unconventional evaluation technique used, is the evaluation on noise and shift invariance.
For this one sample from each class label from the self recorded files of the \enquote{my dataset} is used as test signals.
The length of those audio files is cut, such that by applying a fixed input frame of \SI{500}{\milli\second}, both end positions consists of at least the half of the audio file information, which is especially important for the shift invariance.
The evaluation results are plotted in figures of correct classification upon shift and noise level changes.
Note that the noise and shift invariance are tested on only 5 test signals and therefore this is not a reliable measure for the trained models, but it is interesting to see how different models perform upon this check.
In the following the shift and noise invariance tests are explained in detail.



% --
% shift invariance

\subsubsection{Shift invariance}
Shift invariance is a very important property for speech recognition, for instance the classification of a waveform should still be the same regardless of little shifts in time, as long there is enough and valuable information present that is necessary for a correct classification.
However the restricted frame size of \SI{500}{\milli\second} might increase the difficulty in this task, not all relevant information of the speech signal can be captured by the analytic window, like the \enquote{t} in \enquote{left} or \enquote{right} is often missed.
The figures in this section present a correct classification with a colored pixel and an incorrect with a white pixel.
The examples from the adversarial training section are shown in \rfig{exp_tb_shift_fc3}.
\begin{figure}[!ht]
  \centering
    \subfigure[adv init]{\includegraphics[width=0.45\textwidth]{./5_exp/figs/exp_tb_shift_fc3_adv}}
    \subfigure[random init]{\includegraphics[width=0.45\textwidth]{./5_exp/figs/exp_tb_shift_fc3_random}}
  \caption{Shift invariance of L5-n500, c1d0dd0e0-norm1-it1000-bs32-lr0.0001-mo0.5 once with random init and once with adv init with dec-itl1000.}
  \label{fig:exp_tb_shift_fc3}
\end{figure}
\FloatBarrier
\noindent


% --
% noise invariance

\subsubsection{Noise invariance}
The classification of speech signals often requires noise invariance, because a significant amount of noise can be added from the use of bad microphones or recording set ups and therefore might disturb the classification accuracy.
To create a test upon noise invariance, artificial normal noise is added to the test signal $\bm{x} \in \R^n$ by
\begin{equation}
  \bm{\tilde{x}} = \bm{x} + \bm{v}, \quad \bm{v} \sim \mathcal{N}(\mu, \sigma)
\end{equation}
where $\bm{v} \in \R^n$ is the additive normal noise sampled from $\mathcal{N}(\mu, \sigma)$ with mean $\mu = 0$ and standard deviation $\sigma$.
The additive noise is parametrized with the standard deviation $\sigma$ to create a certain Signal to Noise Ratios (SNR) $S$.
The standard deviation is therefore obtained with:
\begin{equation}
  \sigma = \sqrt{\frac{\frac{1}{n}\bm{x}^T \bm{x}}{10^{\frac{S}{10}}}}
\end{equation}
where $S$ is the SNR in decibel (dB). 
A SNR level of zero means there is equal energy of the added noise $\bm{v}$ and the test signal $\bm{x}$, therefore the resulting signal is already strong disturbed with noise.
In the plots the added noise is indicated in the x-axis with Signal to Noise Ratio (SNR).
\rfig{exp_tb_noise_fc3} shows the noise invariance from the example in the adversarial training section.
\begin{figure}[!ht]
  \centering
    \subfigure[adv init]{\includegraphics[width=0.40\textwidth]{./5_exp/figs/exp_tb_noise_fc3_adv}}
    \subfigure[random init]{\includegraphics[width=0.40\textwidth]{./5_exp/figs/exp_tb_noise_fc3_random}}
  \caption{Noise invariance of L5-n500, c1d0dd0e0-norm1-it1000-bs32-lr0.0001-mo0.5 once with random init and once with adv init with dec-itl1000.}
  \label{fig:exp_tb_noise_fc3}
\end{figure}
\FloatBarrier
\noindent

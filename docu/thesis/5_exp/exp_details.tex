% --
% training details

\section{Implementation and Experiment Details}\label{sec:exp_details}
\thesisStateNotReady
%In this sections, the implementation und parameters for training are described in more detail.
The implementation describes the software tools used for the experiments on the neural networks, such as the programming language and packages for the code.
The training details are provided of commonly used sets of parameters applied in the experiments.
Note that the training details can vary between different experiments and the listing provides merely an overview of all available parameters to choose from and a recommendations of parameters for the speech command dataset.
If parameters are varying in some experiments, they are usually noted, otherwise the parameters listed were used.
The evaluation of the models is described separately, with scores and a self made test bench that evaluates the trained models upon noise and shift invariance.


% --
% implementation notes

\subsection{Implementation Notes}\label{sec:exp_details_implementation}
The code for this thesis was entirely done in \texttt{Python} with version $>3.8$ evaluated on a Linux operating system.
The operating system might be important if someone tries to run the python code on a Windows machine, where unexpected errors might occur (especially regarding path variables).
Also the project does not download the speech command dataset by its own and a path to the dataset has to be specified in the \texttt{config.yaml} file of the project.
More on information about how to used the actual project is described in the \texttt{README.txt} file.
The training and implementation of all used neural networks were done with the \texttt{Pytorch} \cite{Pytorch} framework of version $1.7.0$. 
Usually it should not be a problem if a newer version of \texttt{Pytorch} is used, during the work on the thesis the version changed to $1.9.0$ without any version depending problems.
The feature extraction of Mel Frequency Cepstral Coefficients (MFCC) was self implemented, but use already existing and efficient code for transforms, such as the Short Time Fourier Transform (STFT) or Discrete Cosine Transform (DCT), with packages from \texttt{Scipy}.
All matrix-vector computations were done with the well known package \texttt{Numpy} or with \texttt{Pytorch}.
Several other \texttt{Python} packages were used within the project, but are not named explicitly, they can be looked up in the open source repository of the project if requested.


% --
% training details

\subsection{Neural Network Training Details}\label{sec:exp_details_training}
The training details of the used neural networks described in \rsec{nn_arch} can be split into following parameters:
\begin{enumerate}
  \item Feature extraction parameters
  \item Dataset parameters
  \item Training parameters
  \item Pre-Training details
\end{enumerate}
The feature extraction parameters provide information about how the MFCC features were extracted in detail.
The dataset parameters are the selected labels and the number of examples per labels for training.
With the training parameters, the neural network training is classically specified, such as learning rate or batch size.
The pre-training details are describing a separate training of for instance Generative Adversarial Neural Networks (GAN), to use the obtained weights for Convolutional Neural Networks (CNN).
Therefore the pre-training details are similar to the training parameters but include more specific details, such as which weights were used from the adversarial pre-training.


% --
% feature

\subsubsection{Feature extraction parameters}
During the feature selection experiments in \rsec{exp_fs}, the parameters for cepstral coefficients with enhancements are varying.
If not other stated, the feature extraction parameters in \rtab{exp_details_params_feature} are used.
\input{./5_exp/tables/tab_exp_details_params_feature}


% --
% dataset

\subsubsection{Dataset parameters}
The selected labels are either the 12 labels for comparison to the benchmark networks described in \rsec{prev_kws_benchmark} or 7 labels used for the deployed KWS game are listed as well as the number of examples per label in \rtab{exp_details_params_dataset}.
\input{./5_exp/tables/tab_exp_details_params_dataset}
Note that the amount of examples per label for the 12 and 7 labels differs as shown in \rtab{exp_dataset_all_labels} and the minimum amount of examples per label is provided by the key word \enquote{up} with 2948 examples.
This is important, because the number of examples per label should not be chosen to higher values than 2948 for training, otherwise the same amount of examples per label is not given.


% --
% training parameters

\subsubsection{Training parameters}
The training parameters for the CNNs can be found in ...
\input{./5_exp/tables/tab_exp_details_params_train}


% --
% training parameters

\subsubsection{Pre-Training details}

The pre-training parameters describe the training details of the adversarial networks used for the  described in \rsec{nn_arch_cnn}.
For instance it describes if the weights are used from the discriminator or generator network or how much epochs were used for the adversarial training, etc.
\input{./5_exp/tables/tab_exp_details_params_pre_train}

%\input{./5_exp/tables/tab_exp_details_adv.tex}
%Their selection and references are listed in \rtab{exp_details_train_params}
%The Abbreviations for training parameters can be specified as listed in \rtab{exp_details_adv}
%\input{./5_exp/tables/tab_exp_details_train_params.tex}


% --
% evaluation details

\subsection{Evaulation details and the Test Bench}\label{sec:exp_details_tb}
The main evaluation score of trained models is the computation of the accuracy on the test sets.
The accuracy $a \in [0, 1]$ is simply obtained by counting all correct classifications and deviding it by the number of classified samples $n$.
A score function $c(\hat{y}_i, y_i)$ can be defined as:
\begin{equation}
  c(\hat{y}_i, y_i) = 
  \begin{cases}
    1, & \text{if } \hat{y}_i = y_i\\
    0, & \text{otherwise} 
  \end{cases}
\end{equation}
where $\hat{y}_i \in [0, \dots, C]$ is the predicted label and $y_i \in [0, \dots, C]$ the actual label of the sample $i$ with a total number of classes $C$.
The accuracy can therefore be written as:
\begin{equation}
  a = \frac{\sum_{i=0}^n c(\hat{y}_i, y_i)}{n}
\end{equation}

\subsubsection{Test Bench of best Neural Network Architectures}\label{sec:exp_tb}
\thesisStateNotReady
This section compares the best neural network architectures in terms of noise and shift invariance to fixed self recorded test audio files of the five speech commands with L5 labels.
The length of those audio files is cut, such that by applying a the fixed input frames of \SI{500}{\milli\second}, both end positions consists of at least the half of the audio file information.
This is especially important for the shift invariance.
The noise invariance finds the highest energy region, as described in \rsec{signal_raw} and uses this frame for classification.

% --
% shift invariance

\subsubsection{Shift invariances}
Shift invariances is very important for speech recognition, for instance the waveform should still be classified to the same class, when shifted a little bit in time.
However the restricted frame size of \SI{500}{\milli\second} makes this task very difficult, as it is already known that not all information might fit into the analytic window, like the \enquote{t} in \enquote{left} or \enquote{right} is often missed.
The figures in this section present a correct classification with a colored pixel and an incorrect with a white pixel.
The examples from the adversarial training section are shown in \rfig{exp_tb_shift_fc3}.
\begin{figure}[!ht]
  \centering
    \subfigure[adv init]{\includegraphics[width=0.45\textwidth]{./5_exp/figs/exp_tb_shift_fc3_adv}}
    \subfigure[random init]{\includegraphics[width=0.45\textwidth]{./5_exp/figs/exp_tb_shift_fc3_random}}
  \caption{Shift invariance of L5-n500, c1d0dd0e0-norm1-it1000-bs32-lr0.0001-mo0.5 once with random init and once with adv init with dec-itl1000.}
  \label{fig:exp_tb_shift_fc3}
\end{figure}
\FloatBarrier
\noindent


% --
% noise invariance

\subsubsection{Noise invariances}
Noise invariance is a good trait in the classification of speech signals.
That is because the usage of bad microphones or recording set ups may add a lot of noise to the audio and therefore might disturb the classification accuracy.
To create a test upon noise invariance, artificial normal noise is added to the test audio files.
In the plots this is indicated in the x-axis of the plots as Signal to Noise Ration (SNR).
A SNR level of zero means there is equal energy of noise and signal, therefore this signal is already pretty much disturbed.
\rfig{exp_tb_noise_fc3} shows the noise invariance from the example in the adversarial training section.

\begin{figure}[!ht]
  \centering
    \subfigure[adv init]{\includegraphics[width=0.40\textwidth]{./5_exp/figs/exp_tb_noise_fc3_adv}}
    \subfigure[random init]{\includegraphics[width=0.40\textwidth]{./5_exp/figs/exp_tb_noise_fc3_random}}
  \caption{Noise invariance of L5-n500, c1d0dd0e0-norm1-it1000-bs32-lr0.0001-mo0.5 once with random init and once with adv init with dec-itl1000.}
  \label{fig:exp_tb_noise_fc3}
\end{figure}
\FloatBarrier
\noindent

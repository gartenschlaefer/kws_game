% --
% training details

\section{Implementation and Experiment Details}\label{sec:exp_details}
\thesisStateReady
The implementation describes the software tools used for the experiments on the neural networks, such as the programming language and packages for the source code.
The training details provide commonly used sets of hyperparameters applied to train neural networks.
Note that the training details can vary between different experiments and the listing gives merely an overview of all available parameters to choose from and a recommendations of parameters for the speech command dataset.
If parameters are varying in some experiments, they are usually noted, otherwise the parameters listed were used.
The evaluation of the models is described separately and evaluates the trained models upon accuracy and noise and shift invariance.


% --
% implementation notes

\subsection{Implementation Notes}\label{sec:exp_details_implementation}
The source code for this thesis was entirely done in \texttt{Python} with version $>3.8$ evaluated on a Linux operating system and is open source available in \cite{KWSGame}.
The operating system might be important if someone tries to run the python code on a Windows machine, where unexpected errors might occur (especially regarding path variables).
Also the project does not download the speech command dataset by its own and a path to the dataset has to be specified in the \texttt{config.yaml} file of the project.
More on information about how to run the project is described in the \texttt{README.txt} file.
The training and implementation of all used neural networks were done with the \texttt{Pytorch} \cite{Pytorch} framework of version $1.7.0$. 
Usually it should not be a problem if a newer version of \texttt{Pytorch} is used, during the work on the thesis the version changed to $1.9.0$ without any version depending problems.
The feature extraction of Mel Frequency Cepstral Coefficients (MFCC) was self implemented, but use already existing and efficient code for transforms, such as the Short Time Fourier Transform (STFT) or Discrete Cosine Transform (DCT), with packages from \texttt{Scipy}.
All matrix-vector computations were done with the well known package \texttt{Numpy} or with \texttt{Pytorch}.
Several other \texttt{Python} packages were used within the project, but are not named explicitly, they can be looked up in the open source repository of the project, if requested.


% --
% training details

\subsection{Neural Network Training Details}\label{sec:exp_details_training}
The training details of the used neural networks described in \rsec{nn_arch} can be split into following parameters:
\begin{enumerate}
  \item Feature extraction parameters
  \item Dataset parameters
  \item Training parameters
  \item Pre-Training details
\end{enumerate}
The feature extraction parameters provide information about how the MFCC features were extracted in detail.
The dataset parameters are the selected labels and the number of examples per labels for training.
With the training parameters, the neural network training is classically specified, such as learning rate or batch size.
The pre-training details are describing a separate training of for instance Generative Adversarial Neural Networks (GAN), to use the obtained weights for Convolutional Neural Networks (CNN).
Therefore the pre-training details are similar to the training parameters but include more specific details, such as which weights were used from the adversarial pre-training.


% --
% feature

\subsubsection{Feature extraction parameters}
During the feature selection experiments in \rsec{exp_fs}, the parameters for cepstral coefficients with enhancements are varying.
If not other stated, the feature extraction parameters in \rtab{exp_details_params_feature} are used.
\input{./5_exp/tables/tab_exp_details_params_feature}


% --
% dataset

\subsubsection{Dataset parameters}
The selected labels for training are either the 12 labels (L12) for comparison to the benchmark networks described in \rsec{prev_kws_benchmark} or 7 labels (L7) used for the deployed KWS game are listed as well as the number of examples per label in \rtab{exp_details_params_dataset}.
\input{./5_exp/tables/tab_exp_details_params_dataset}
Note that the amount of examples per label for the 12 and 7 labels differs as shown in \rtab{exp_dataset_all_labels} and the maximal amount of examples per label for training is chosen by the minimum examples per selected label for each label and each set.
This is provided by the validation set of \enquote{go} with \{train: 2948, test: 425, validation: 350 \} with 350 examples per label, considering a 10\% split.
This gives the maximal amount of 3500 examples
% provided by the key word \enquote{up} with 2948 examples.



This is important, because the number of examples per label should not be chosen to higher values than 2948 for training, otherwise the same amount of examples per label is not given.


% --
% training parameters

\subsubsection{Training parameters}
The training parameters for the CNNs can be found in ...
\input{./5_exp/tables/tab_exp_details_params_train}


% --
% training parameters

\subsubsection{Pre-Training details}

The pre-training parameters describe the training details of the adversarial networks used for the  described in \rsec{nn_arch_cnn}.
For instance it describes if the weights are used from the discriminator or generator network or how much epochs were used for the adversarial training, etc.
\input{./5_exp/tables/tab_exp_details_params_pre_train}


% --
% evaluation details

\subsection{Evaluation details}\label{sec:exp_details_tb}
The main evaluation score of the trained models is the computation of the accuracy on the test sets described in \rsec{exp_dataset}.
The accuracy is simply obtained by counting all correct classifications and dividing it by the number of classified samples $n$.
A score function $c(\hat{y}_i, y_i)$ for the accuracy can be defined as:
\begin{equation}
  c(\hat{y}_i, y_i) = 
  \begin{cases}
    1, & \text{if } \hat{y}_i = y_i\\
    0, & \text{otherwise} 
  \end{cases}
\end{equation}
where $\hat{y}_i \in \{0, 1, \dots, L\} = \mathcal{L}$ is the predicted label and $y_i \in \mathcal{L}$ the actual label of the sample $i$ with a total number of class labels $L$.
The accuracy $a \in [0, 1]$ can therefore be written as:
\begin{equation}
  a = \frac{\sum_{i=0}^n c(\hat{y}_i, y_i)}{n}
\end{equation}
Another more unconventional evaluation technique used, is the evaluation on noise and shift invariance.
For this one sample from each class label from the self recorded files of the \enquote{my dataset} is used as test targets.
The length of those audio files is cut, such that by applying a fixed input frame of \SI{500}{\milli\second}, both end positions consists of at least the half of the audio file information, which is especially important for the shift invariance.
The evaluation results are plotted in figures of correct classification upon shift and noise level changes.
In the following the shift and noise invariance test is explained in detail.



% --
% shift invariance

\subsubsection{Shift invariance}
Shift invariance is very important for speech recognition, for instance the classification of a waveform should still be the same regardless of little shifts in time, as long there is enough and valuable information present that is necessary for a correct classification.
However the restricted frame size of \SI{500}{\milli\second} might increase the difficulty in this task, not all relevant information of the signal might fit into the analytic window, like the \enquote{t} in \enquote{left} or \enquote{right} is often missed.
The figures in this section present a correct classification with a colored pixel and an incorrect with a white pixel.
The examples from the adversarial training section are shown in \rfig{exp_tb_shift_fc3}.
\begin{figure}[!ht]
  \centering
    \subfigure[adv init]{\includegraphics[width=0.45\textwidth]{./5_exp/figs/exp_tb_shift_fc3_adv}}
    \subfigure[random init]{\includegraphics[width=0.45\textwidth]{./5_exp/figs/exp_tb_shift_fc3_random}}
  \caption{Shift invariance of L5-n500, c1d0dd0e0-norm1-it1000-bs32-lr0.0001-mo0.5 once with random init and once with adv init with dec-itl1000.}
  \label{fig:exp_tb_shift_fc3}
\end{figure}
\FloatBarrier
\noindent


% --
% noise invariance

\subsubsection{Noise invariance}
Noise invariance is especially important in the classification of speech signals, because of the usage of bad microphones or recording set ups noise is added to the signal and therefore might disturb the classification accuracy.
To create a test upon noise invariance, artificial normal noise is added to the test audio files.
In the plots this is indicated in the x-axis of the plots as Signal to Noise Ration (SNR).
A SNR level of zero means there is equal energy of noise and signal, therefore this signal is already strong disturbed with noise.
\rfig{exp_tb_noise_fc3} shows the noise invariance from the example in the adversarial training section.

\begin{figure}[!ht]
  \centering
    \subfigure[adv init]{\includegraphics[width=0.40\textwidth]{./5_exp/figs/exp_tb_noise_fc3_adv}}
    \subfigure[random init]{\includegraphics[width=0.40\textwidth]{./5_exp/figs/exp_tb_noise_fc3_random}}
  \caption{Noise invariance of L5-n500, c1d0dd0e0-norm1-it1000-bs32-lr0.0001-mo0.5 once with random init and once with adv init with dec-itl1000.}
  \label{fig:exp_tb_noise_fc3}
\end{figure}
\FloatBarrier
\noindent

% --
% training details

\section{Implementation and Experiment Details}\label{sec:exp_details}
The implementation details describe the used software tools in the experiments, such as the programming language and packages for the source code.
The experiment details are separated into training details of the used neural networks and evaluation details of the trained models.
The training details provide the used sets of hyperparameters for the training of the neural networks on the speech commands dataset.
Note that the training details can vary in different experiments.
If they do so, they are usually noted in the description of the corresponding experiment, otherwise the parameters listed are used.
The evaluation details describe how the trained models are evaluated with respect to classification accuracy and noise and shift invariance.


% --
% implementation notes

\subsection{Implementation Notes}\label{sec:exp_details_implementation}
The source code for this thesis was written in \texttt{Python} with version $>3.8$ and tested on a Linux operating system.
Further, the source code is open source available in \cite{KWSGame}.
The operating system might be relevant if someone tries to run the python code on a \enquote{Windows} machine, where unexpected errors might occur (especially regarding path variables).
Note that the project does not explicitly download the speech commands dataset.
A path variable to the downloaded dataset has to be specified in the \texttt{config.yaml} file of the project, where users may also change several other useful parameters.
More information about how to run the project is described in the \texttt{README.txt} file.
The training and implementation of all used neural networks were done with the \texttt{Pytorch} \cite{Paszke2019Pytorch} framework of version $>1.7.0$. 
The feature extraction of MFCCs was self implemented but used already existing and efficient code for signal transformations, such as the STFT or DCT provided by the \texttt{Scipy} package.
All matrix-vector computations were processed with the well known \texttt{Numpy} package or with \texttt{Pytorch}.
Several other \texttt{Python} packages were used within the project but are not explicitly mentioned.
They can be looked up in the open source repository of the project, if requested.


% --
% training details

\subsection{Neural Network Training Details}\label{sec:exp_details_training}
The training details and parameters of the used neural networks can be split into following components: Feature extraction, dataset, training, and pre-training.
The feature extraction parameters provide information about the extraction of MFCC features.
The dataset parameters are specified by the selected labels and the number of examples per labels for training.
The training part lists hyperparameters, such as the learning rate or the batch size.
Finally, the pre-training details describe a separate training of GANs to use the obtained weights for transfer learning on an equivalent CNN.


% --
% feature

\subsubsection{Feature Extraction Parameters}
The MFCC feature extraction parameters are listed in \rtab{exp_details_params_feature}.
Note that some parameters can vary in the experiments.
\input{./5_exp/tables/tab_exp_details_params_feature}


% --
% dataset

\subsubsection{Dataset Parameters}
The labels for training are selected to the same 12 labels (L12) that were also used in the benchmark models in \rsec{prev_kws_benchmark}.
Therefore, the L12 labels ensure a valid comparison between the performed experiments in this section and the benchmark models.

The maximum number of examples per label for training and evaluation is given by the minimum examples of each selected label in each set.
This is provided by the label \enquote{go} with following amount of examples in the sets: \{train: 2948, test: 425, validation: 350 \} from \rtab{exp_dataset_all_labels}.
Hence, the validation set consists of 350 examples of the label \enquote{go}.
By considering a \SI{10}{\percent} split of both test and validation set, this gives a maximum amount of 3500 examples per label to represent the whole dataset.
Therefore, the number of examples per label should not exceed the value 3500, otherwise the equal amount of examples per label is not given.
Note that in order to reduce computations in some experiments, the number of examples were reduced to only 500 per label.
\rtab{exp_details_params_dataset} lists the L12 labels and their number of examples per label used for training.
\input{./5_exp/tables/tab_exp_details_params_dataset}
Note that the actions set of the deployed video game in \rsec{game} uses only a subset of the L12 labels, namely the set: \{\enquote{left},  \enquote{right}, \enquote{up}, \enquote{down}, \enquote{go}, \enquote{\_mixed}, \enquote{\_noise}\}.
Still the video game operates with the models trained on the L12 labels and thus might increase the chance in confusing keywords.


% --
% training hyperparameters

\subsubsection{Training Hyperparameters}
\rtab{exp_details_params_train} shows the hyperparameters for the training of the used CNNs models, described in \rsec{nn_arch}.
\input{./5_exp/tables/tab_exp_details_params_train}
From the following experiments, it can be observed that epochs of 2000 result into small overfitting effects regarding certain models.
This might decrease the recognition accuracy if the set of parameters is used at the last epoch and no early stopping mechanism is applied.
The batch size of 32 is selected to a low number because it worked well on the KWS task of speech commands.
Further, the amount of classes is at maximum 12 (L12 labels) and therefore a batch size of 32 would include most of the individual labels.
Note that the hyperparameters for the Wavenet model are described in the corresponding experiment section.


% --
% training parameters

\subsubsection{Pre-Training Details}
The pre-training parameters describe the training of the GANs with their models presented in \rsec{nn_arch_adv}.
The hyperparameters listed in \rtab{exp_details_params_pre_train}, are the same as for usual training but the Discriminator and Generator network can have each different values.
\input{./5_exp/tables/tab_exp_details_params_pre_train}
The selection of the epochs is important, as it was already pointed out in \rsec{nn_adv}.


% --
% evaluation details

\subsection{Evaluation Details}\label{sec:exp_details_tb}
The main evaluation score of the trained models is the classification accuracy on the test sets.
The accuracy is obtained by counting all correct classifications and dividing it by the number of classified samples $n$.
A score function $c(\hat{y}_i, y_i)$ can be defined as
\begin{equation}
  c(\hat{y}_i, y_i) = 
  \begin{cases}
    1, & \text{if } \hat{y}_i = y_i\\
    0, & \text{otherwise} 
  \end{cases},
\end{equation}
where $\hat{y}_i \in \mathcal{L} = \{0, 1, \dots, L - 1\} $ is the predicted label and $y_i \in \mathcal{L}$ the actual label of the sample $i$ with a total number of class labels $L$.
The classification accuracy $a \in [0, 1]$ therefore formulates as
\begin{equation}
  a = \frac{1}{n} \sum_{i=0}^{n-1} c(\hat{y}_i, y_i).
\end{equation}
Another, more unconventional evaluation technique, is the evaluation upon noise and shift invariance of dedicated test signals.
For this, one example of each class label was taken from the self recorded files of the \enquote{my dataset} and used as test signal.
The length of those audio files is cut such that by applying a fixed input frame of \SI{500}{\milli\second} both end positions consist of at least the half of the audio file's information.
This is especially relevant to the shift invariance test.
The evaluation results are plotted in figures of correct classification upon noise level and shift index changes of the test signals.
Note that the noise and shift invariance are tested on only 5 test signals.
Therefore, they are not a reliable measure for the trained models.
Yet it is interesting to observe how different models perform upon these tests.
In the following, the shift and noise invariance tests are explained in more detail.


% --
% shift invariance

\subsubsection{Shift Invariance}
Shift invariance is a very important property in speech recognition tasks. 
For instance, a waveform of a speech command should still be classified to the same label regardless of little shifts in time as long as the analytic window includes sufficiently valuable information of this speech command.
However, the analytical window size of merely \SI{500}{\milli\second} might increase the difficulty in this task.
Not all relevant information of the speech signals can be captured by the analytic window, like the \enquote{t} in \enquote{left} or \enquote{right} is often missed when the speaker prolongs those words.
An example of the application of the shift invariance test is shown in \rfig{exp_details_tb_shift_left} with a beginning, middle, and end frame shift.
\begin{figure}[!ht]
  \centering
    \subfigure[frame shift 0]{\includegraphics[width=0.48\textwidth]{./5_exp/figs/exp_details_tb_shift_left_frame0.png}}
    \quad
    \subfigure[frame shift 30]{\includegraphics[width=0.48\textwidth]{./5_exp/figs/exp_details_tb_shift_left_frame30.png}}
    \subfigure[frame shift 59]{\includegraphics[width=0.48\textwidth]{./5_exp/figs/exp_details_tb_shift_left_frame59.png}}
  \caption{Shifting a self recorded example of the label \enquote{left} with certain amounts of frame shifts. The classification results are provided in the title annotations.}
  \label{fig:exp_details_tb_shift_left}
\end{figure}
\FloatBarrier
\noindent
The figures in this section present a correct classification with a colored pixel and an incorrect with a white one.
\rfig{exp_details_tb_shift} provides one example of a shift invariance test.
\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.65\textwidth]{./5_exp/figs/exp_fs_cepstral_tb_shift_conv-jim_mfcc12_norm0.png}
  \caption{Shift invariance test example selected from one of the trained models in the experiments.}
  \label{fig:exp_details_tb_shift}
\end{figure}
\FloatBarrier
\noindent
The purpose of the shift invariance test is not to achieve a full classification score upon each test example because this is hardly possible if not all of a keyword's information is captured within the shifted frame.
Conversely, the shift invariance test analyzes whether there are consecutive correct classifications within a certain region of shift indices.
Holes in this region are not a good indicator for the trained model.
If one example could not be classified at all, it does not necessarily imply that the trained model performs poorly but that this specific example is not recognized with this specific training instance of the model.
A well performing model usually has a wide region of correct classifications with no holes in it.


% --
% noise invariance

\subsubsection{Noise Invariance}
The classification of speech signals often requires noise invariance.
This is because a significant amount of noise that is added, for instance, through the use of poor microphones or recording set-ups, might disturb the classification accuracy.
To construct a test upon noise invariance, AWGN is added to the test signal $\bm{x} \in \R^n$ by
\begin{equation}
  \bm{\tilde{x}} = \bm{x} + \bm{v}, \quad \bm{v} \sim \mathcal{N}(\mu, \sigma),
\end{equation}
where $\bm{v} \in \R^n$ is the additive noise sampled from the normal distribution $\mathcal{N}(\mu, \sigma)$ with mean $\mu = 0$ and standard deviation $\sigma$.
The AWGN is parametrized through the standard deviation $\sigma$ to create a certain Signal to Noise Ratio (SNR), denoted as $S$.
Rewriting the formula of the SNR, the standard deviation can be obtained by
\begin{equation}
  \sigma = \sqrt{\frac{\frac{1}{n}\bm{x}^T \bm{x}}{10^{\frac{S}{10}}}}
\end{equation}
for requested SNR values $S$ in decibel (dB).
A SNR level of zero means that there is equal energy of the added noise $\bm{v}$ and the test signal $\bm{x}$, therefore, the resulting signal is already strong disturbed with noise.
\rfig{exp_details_tb_noise_left} provides an example of the noise invariance test with a low, middle, and high SNR value.
\begin{figure}[!ht]
  \centering
    \subfigure[\SI{16}{\dB}]{\includegraphics[width=0.48\textwidth]{./5_exp/figs/exp_details_tb_noise_left_snr16.png}}
    \quad
    \subfigure[\SI{0}{\dB}]{\includegraphics[width=0.48\textwidth]{./5_exp/figs/exp_details_tb_noise_left_snr0.png}}
    \subfigure[\SI{-16}{\dB}]{\includegraphics[width=0.48\textwidth]{./5_exp/figs/exp_details_tb_noise_left_snr-16.png}}
  \caption{Adding noise to a self recorded example of \enquote{left} with certain SNR values. The classification results are provided in the title annotations.}
  \label{fig:exp_details_tb_noise_left}
\end{figure}
\FloatBarrier
\noindent
The plots for the noise invariance test indicate the added noise through the SNR value on the x-axis.
\rfig{exp_details_tb_noise} shows an example of a noise invariance test.
\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.35\textwidth]{./5_exp/figs/exp_fs_cepstral_tb_noise_conv-jim_mfcc12_norm0.png}
  \caption{Noise invariance test example selected from the trained models in the experiments.}
  \label{fig:exp_details_tb_noise}
\end{figure}
\FloatBarrier
\noindent
The same criteria, as described in the shift invariance, also applies to the noise invariance, where the region of consecutive correct classifications should start from low noise levels to high noise levels.
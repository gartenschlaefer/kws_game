% --
% dataset

\section{Dataset}\label{sec:exp_dataset}
\thesisStateReady
Two datasets are used within this thesis, one is the second version of the speech commands dataset from \cite{Warden2018} and one is self made, denoted as \enquote{my dataset} which consists of only 5 labels that are especially valuable for movement in video games.
Note that the \enquote{my dataset} is merely used for evaluation.
The training, validation and testing of the neural network architectures is done on the speech commands dataset.
Both datasets consists of raw waveform files in the \texttt{.wav} format, no feature extraction was done beforehand.
As already mentioned in \rsec{prev_kws_benchmark} direct comparisons between different neural network approaches is difficult if the feature extraction is left to the user alone.
Some datasets provide feature extraction beforehand, so that the comparability of neural network architectures performances is not influenced on it.
The speech commands dataset does not explicitly separate each \texttt{.wav} file into train, test and validation sets, but provides file lists that refers to distinct waveform files that should be used for testing and validation and therefore gives a guidline for comparison.
More details of the datasets are presented below.

% Some abbreviations and references were done, so that the jungle of selected parameters get a little bit more clear to the reader of this thesis.in the \texttt{.wav} format
% The abbreviations of the dataset are shown in \rtab{exp_dataset_abbr}.

% The speech commands dataset is extracted before it is used for training. 
% To reduce computations in the evaluation process of neural networks, it was important to reduce the number of classes and examples per class to an suitable number.

% \input{./5_exp/tables/tab_exp_dataset_abbr.tex}


% --
% speech commands dataset

\subsection{Speech Commands Dataset}\label{sec:exp_dataset_speech_cmd}
The speech command dataset \cite{Warden2018} exists in two versions (\texttt{v0.01} and \texttt{v0.02}), the first one was published in 2017 and the second one emerged as improved version of \texttt{v0.01} with 5 more key words \{\enquote{backward}, \enquote{forward}, \enquote{follow}, \enquote{learn}, \enquote{visual}\}, class examples and quality in 2018.
The  dataset consists of \SI{1}{\second} speech recordings of 30 different words in \texttt{v0.01} and 35 different words in \texttt{v0.02}, done by over thousands of individual speakers.

In this thesis, the experiments are done on the second version \texttt{v0.02} of the dataset.
The hard-facts about the speech command dataset \texttt{v0.02} are listed in \rtab{exp_dataset_hard_facts}.
% hard facts
\input{./5_exp/tables/tab_exp_dataset_hard_facts.tex}
All speech command key words with their separation in training, test and validation set are shown in \rtab{exp_dataset_all_labels} loacated in the appendix.
% all labels tab
%\input{./5_exp/tables/tab_exp_dataset_all_labels.tex}
In \rtab{exp_dataset_all_labels} it can be observed, that some labels are occurring significantly more often than others.
The idea behind this is, as described in \cite{Warden2018}, to separate the key words into \emph{core key words} and \emph{auxiliary key words}.
The core key words are the more frequent ones with about 3000 to 4000 examples each, in detail they are \{\enquote{yes}, \enquote{no}, \enquote{up}, \enquote{down}, \enquote{left}, \enquote{right}, \enquote{on}, \enquote{off}, \enquote{stop}, \enquote{go}, \enquote{zero}, \enquote{one}, ..., \enquote{nine} \}.

The auxiliary words are the less frequent ones that are not already listed above in the core key words of all available key words in the dataset.
The intention is that the core key words should each get an own class label and classified individually out of all key words.
The auxiliary words are used as nuisances and should gather to a single representative class label for all other possible unknown words to the classification system.
Those auxiliary key words are often labeled in papers with \enquote{unknown}, but in this thesis it is referred as \enquote{\_mixed} label, since it is a mixture of auxiliary key words.
Note that the auxiliary key words make the classification task much more difficult because of similarities to the core words.
In the creation of the examples for the \enquote{\_mixed} label, it is important to have equal amounts of examples per individual auxiliary key word.

Some examples of the speech command dataset in raw audio format are shown in \rfig{exp_dataset_wav_grid_speech_commands_v2} to provide a small glimpse on the recordings.
\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.65\textwidth]{./5_exp/figs/exp_dataset_wav_grid_speech_commands_v2}
  \caption{One random sample of each individual speech command in the speech command dataset in normalized raw audio format.}
  \label{fig:exp_dataset_wav_grid_speech_commands_v2}
\end{figure}
\FloatBarrier
\noindent


% --
% statistics

\subsubsection{Observations of all examples in the dataset}
Two histograms were created to observe the quality of all recorded files, those are an energy measure for each file and the count of the sample length.
The energy of a recorded file provides information about if a recording is too silent (background noise only) or too loud (overdrive distortions). In the first version of the speech command dataset (\texttt{v0.01}) too silent files were an issue.
Luckily this was solved in the second version by rejecting those silent files.
The energy value $e \in \R$ is computed as:
\begin{equation}\label{eq:exp_dataset_energy}
  e = \frac{1}{n} \left( x\, x^T \right)
\end{equation}
where $x \in \R^n$ are the values of all samples from the recorded files.
The division through the length each individual file in samples $n$, has to be done, because not every file has a \SI{1}{\second} duration with 16000 samples.
The sample length is simply the amount of samples in a recording file, those ideally should have a duration of \SI{1}{\second}, but for some unknown reason some of the recordings have less.
It would be problematic if the sample length is too low to capture a word, but the minimum duration of all files is about \SI{0.4}{\second}, and this is enough for words like \enquote{go}, etc.
The histograms of all available examples in the dataset are shown in \rfig{exp_dataset_hist}.

\begin{figure}[!ht]
  \centering
    \subfigure[energy]{\includegraphics[width=0.45\textwidth]{./5_exp/figs/exp_dataset_hist_energy_overall}}
    \subfigure[sample length]{\includegraphics[width=0.45\textwidth]{./5_exp/figs/exp_dataset_hist_sample_overall}}
  \caption{Energy value and sample length histograms in log-log and log-scale respectively of all examples in the speech commands dataset \texttt{v0.02}.}
  \label{fig:exp_dataset_hist}
\end{figure}
\FloatBarrier
\noindent
The energy histogram has one main lobe, which is good, so it means that there are no clusters of extremely silent or loud files.
For comparison, if silent files were extracted from for instance the given background files, the energy of some of those is in range of $10^{-7} \dots 10^{-6}$. Therefore it seems that most of the files are okay.
The sample length histogram shows, that most of the files have a duration of \SI{1}{\second}, but many other have less sample numbers. 
This is important and has to be regarded in the pre-processing of audio files, because the neural networks inputs are usually a fixed size input, if no sequential neural networks are deployed, such as Recurrent Neural Networks (RNN).


% --
% recording quality

\subsubsection{Recording Quality and Personal Experience}
The examples within the dataset were not recorded by professionals with high-end recording equipment, in fact the recordings had been done in an amateur kind of fashion, so that the dataset is more suited to realistic environments intended for user applications.
This is also noted in the paper \cite{Warden2018}:
\begin{quote}
...This meant that the use of studio-captured samples seemed unrealistic, since that audio would lack background noise, would be captured with high-quality microphones, and in a formal setting. 
Successful models would need to cope with noisy environments, poor quality recording equipment, and people talking in a natural, chatty way...
\end{quote}
The recording devices of the speakers, who contributed examples to the dataset, were in most cases simple consumer microphones, as for instance deployed in laptops or mobile phones.

The personal experiments made, when listening to the examples in the dataset, were:
\begin{itemize}
  \item The quality of the examples in the dataset are ranging from really good and understandable to very bad, noisy, unrecognizable and cut off, though most of the examples are good.

  \item Different accents can be perceived, that suggests that people from several countries were involved. 
  However the bias was laid on American English, as noted in the paper.

  \item No children speakers had been found in the personal listening of examples.
\end{itemize}

Due to data privacy issues the information of individual speakers is not displayed.
Further it is not clear if there are equal amounts of male and female speakers and if there are any children speakers included.
The last would be especially interesting for a video games suited for kids.

In many recordings the background noise is imminent, such as traffic noise, chattering people, office sounds, etc.
A quality check of the recorded files in the dataset had been done beforehand, as described in \cite{Warden2018}, to ensure that bad samples were rejected.
However there are still some existing flaws such as extremely loud or silent files or examples with inconsistent sample numbers or too much noise in it or in the worst case, noise only (very rarely).
Those quality issues in the dataset are for most cases neglectable or can be fixed, such as inconsistent sample numbers. 
Other more problematic cases, for instance noise-only examples, should actually be filtered out, but since their occurrence is very rare, it is not worth the effort.
Usually it is not a problem for neural networks to cope with noisy datasets. 
Actually it is favorable if the dataset contains many noisy samples, so that neural networks can learn invariance against noise, loudness differences and other nuisances during training.
Further if the training dataset is large enough and the test and validation sets do not contain very bad examples, there should be no problem in training and evaluation of different models.
%Still it is a great dataset, because there is no need for a perfect dataset when working with neural networks and one can be happy that there exists one with this amount of diversity and free of access under the creative common license.
%And since the dataset provides references for testing and validation files, those files surely were checked, so that no extremely bad example is included.


% --
% dataset structure

\subsubsection{Dataset Structure}
The speech command examples are stored in separate folders named after each individual key word, in the \texttt{.wav} format.
The folder named as \texttt{\_background\_noise\_} contains six different background noise files, such as \texttt{white\_noise.wav} or \texttt{doing\_the\_dishes.wav}, with a duration of more than one minute each.
Noise examples with a new noise label named \texttt{\_noise} were extracted from those background noise files with a \SI{1}{\second} window shifted by \SI{0.2}{\second}.

Each waveform file is named with an 8-digit hexadecimal hash code for the speaker identification, followed by the utterance number, for instance \texttt{3b4f8f24\_nohash\_0.wav}.
Therefore it is possible to distinguish between different speakers, however as mentioned above, no further information about the speaker is given due to data privacy issues.

Further the dataset provides a testing file list called \texttt{testing\_list.txt} and a validation file list \texttt{validation\_list.txt} where each row entry refers to a file in the dataset, for example \texttt{right/bb05582b\_nohash\_3.wav}.
Those file lists for testing and validation should ensure the comparability between different neural network approaches from individual researchers.
The testing and validation file lists are applied in this thesis and the separation into sets already shown in \rtab{exp_dataset_all_labels}.


% --
% my dataset

\subsection{My own Dataset}\label{sec:exp_dataset_my}
This dataset was created by the author of this thesis and contains five examples samples each from the words \{\enquote{left}, \enquote{right}, \enquote{up}, \enquote{down} and \enquote{go}\}.
The datasets purpose is mainly to have an additional test set for evaluating trained models on the authors own voice with different word pronouncement on each example.
It is important to mention that none of the self recorded files were used within the training set, so that the neural networks performance on this unseen data is evaluated.
All examples of my own dataset are illustrated in \rfig{exp_dataset_wav_grid_my} in raw audio format.
\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.65\textwidth]{./5_exp/figs/exp_dataset_wav_grid_my}
  \caption{Self recorded files of the \enquote{my dataset} in raw audio format.}
  \label{fig:exp_dataset_wav_grid_my}
\end{figure}
\FloatBarrier
\noindent
The examples per word are spoken with different emphasis and stress on individual phonemes.
Also the prolongation of the words are different, that in one example the word is fast and in the other its slow spoken.
The emphasis and prolongation ensure the diversity of the dataset. 
It turned out that it is not easy for neural networks to gain a $100\%$ classification score upon it, even though there is only one and the same speaker.


% --
% preparation for neural networks

\subsection{Data preparation for neural networks}\label{sec:exp_data_prep}
The neural networks architectures are trained with supervised learning, that means a class label $y_i$ correspondence to each data example $x_i$ must exist.
Some selected examples and their labels form a dataset $S$ for example for training or testing and can be written as:
\begin{equation}\label{eq:exp_dataset}
  S = \{ (\,x_i, \, y_i\,) \quad | \, i = 0 \dots n \}
\end{equation}
where $n$ is the total number of examples within the dataset.
Class labels $y_i$ are usually translated to integer numbers that are referencing to indices in a class vocabulary, for instance $y_1 = 0$ refers to the label \enquote{left} in the indexed vocabulary \{0: \enquote{left}, 1: \enquote{right}\}.
It is important that the enumeration of class labels starts with zero, because they refer to the output nodes for each class in the used neural networks.

It was already shown how to extract MFCC features in \rsec{signal_mfcc}, however it is important that each individual $x_i$ for all $i$ has the same dimension, because of data preparation for training with neural networks.
It could happen that the sample numbers of the waveform files are inconsistent as described in \rsec{exp_dataset_speech_cmd} and therefore a different dimension is obtained.
To ensure that each $x_i$ has the same dimension, the audio files were adjusted to have the same sample length of a duration of \SI{1}{\second} with sampling frequency \SI{16}{\kilo\hertz}.
This was done by zero-padding the signals to the desired length of 16000 samples.
Further dither noise was added, so that neural networks are not confused when seeing pure zeros emerging from the data examples.
The dithering was done by Gaussian additive noise with:
\begin{equation}\label{eq:exp_dither}
  x = \tilde{x} + v, \quad v = \mathcal{N}(\mu=0, \sigma=0.5) \cdot \tilde{x}_{quant}%, \quad \mu, \sigma = 0, 0.5
\end{equation}
where $\mathcal{N}$ is the normal distribution, $\tilde{x} \in \R^n$ the zero-padded signal (if the sample numbers were incorrect) and $\tilde{x}_{quant} \in \R$ the quantization error, corresponding to the minimal absolute value of all samples of $\tilde{x}$ except the pure zero entries.
When the dithering is applied to the signal, no pure zeros exist anymore with less altering of the original signal, because the maximum change lies in a normal range of the quantization noise from the original recording quantization.

